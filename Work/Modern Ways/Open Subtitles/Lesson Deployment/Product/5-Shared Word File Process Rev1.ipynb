{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Word File Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "#import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool, Queue\n",
    "from itertools import islice\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nprocs = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {nprocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Spanish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "lang_pair = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# adding native word to shared word\n",
    "word_start = 0  # 0  # native word start index\n",
    "word_end = 28  # 28  # native word end index\n",
    "\n",
    "# shared word frequency\n",
    "shared_word_frequency = True  # True, False\n",
    "\n",
    "# prefix suffix file\n",
    "prefix_suffix = False  # True, False  # always must be False in this part\n",
    "native_word = True # True for adding native word\n",
    "etymology_word = True  # True for adding etymology word\n",
    "\n",
    "# adding output file extention\n",
    "if (not prefix_suffix) & etymology_word & native_word:\n",
    "    file_ext = \"1\"\n",
    "elif (not prefix_suffix) & etymology_word & (not native_word):\n",
    "    file_ext = \"2\"\n",
    "elif prefix_suffix & etymology_word & native_word:\n",
    "    file_ext = \"3\"\n",
    "elif prefix_suffix & etymology_word & (not native_word):\n",
    "    file_ext = \"4\"\n",
    "elif prefix_suffix & (not etymology_word) & native_word:\n",
    "    file_ext = \"5\"\n",
    "elif (not prefix_suffix) & (not etymology_word) & native_word:\n",
    "    file_ext = \"6\"\n",
    "else:\n",
    "    file_ext = \"7\"              \n",
    "# 1 => for native word and etymology word without prefix suffix. \n",
    "# 2 => for only etymology word without prefix suffix. \n",
    "# 3 => for native word and etymology word with prefix suffix. prefix_suffix, native_word and etymology_word must be True. \n",
    "# 4 => for only etymology word with prefix suffix.\n",
    "# 5 => for only native word with prefix suffix.\n",
    "# 6 => for only native word without prefix suffix.\n",
    "print(f\"{file_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twogram In Threegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.lower().capitalize()}/Deployment/Data/Word/Word_Merge_Preprocess.xlsx\")\n",
    "df_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_file = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/4-Shared Word/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Result_With_Frequency{file_ext}.xlsx\")\n",
    "#df_shared_file = pd.read_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Result_With_Frequency.xlsx\")\n",
    "df_shared_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_result(df,column_list): # df is dataframe, column_list is list value\n",
    "    '''\n",
    "    word_count_bool(df, column_list): df columns word count for word frequency\\n\n",
    "    df is dataframe, column_list is list value\\n\n",
    "    word_count_bool(df, [\"word\",\"twogram\"]):\n",
    "    '''\n",
    "    list_all = []\n",
    "    for i in df.loc[:,[x for x in column_list]].columns:\n",
    "        var_list = df[f\"{i}\"].dropna().tolist()\n",
    "        for j in var_list:\n",
    "            list_all.append(j)\n",
    "    text = \" \".join(list_all)\n",
    "    word_list = re.findall(r\"\\w+\",text, re.UNICODE)\n",
    "    df_word_list = pd.DataFrame(word_list, columns=[\"word\"])\n",
    "    #df_word_list.rename(columns={0:\"word\"}, inplace=True)\n",
    "    df_word_count = pd.DataFrame(df_word_list.value_counts())\n",
    "    df_word_count.reset_index(inplace=True)\n",
    "    df_word_count.rename(columns={0:\"word_count\"}, inplace=True)\n",
    "    \n",
    "    return  df_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_in_wordgroup(df, list_column, target_column):\n",
    "\n",
    "    '''word_in_wordgroup(df, \"word\", \"twogram\"):\n",
    "       df is dataframe, list_column and target_column are \n",
    "       dataframe column string name. list_column convert list\n",
    "       values that are in target column.\n",
    "    '''\n",
    "    \n",
    "    df_select = df[[f\"{target_column}\"]].dropna()\n",
    "    df_word_result = pd.DataFrame()\n",
    "    for i in df[f\"{list_column}\"].dropna():\n",
    "        try:\n",
    "            #word_in_twogram = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)].head(10)  # Option\n",
    "            word_in_word_cluster = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)].head(100) \n",
    "        except:\n",
    "            pass        \n",
    "        word_in_word_cluster.insert(0,f\"{list_column}\",i)\n",
    "        df_word_result = pd.concat([df_word_result,word_in_word_cluster], axis=0)\n",
    "    df_word_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_word_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_shared_count = word_count_result(df_shared_file,[\"threegram\"])\n",
    "#df_shared_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_file[\"twogram\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two_in_three = word_in_wordgroup(df_shared_file, \"twogram\", \"threegram\")\n",
    "df_two_in_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two_in_three[\"twogram\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_select_twogram = df_shared_file.loc[:,[\"twogram\",\"freq_twogram\"]]\n",
    "df_shared_select_twogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_shared_twogram = set(df_shared_select_twogram[\"twogram\"])\n",
    "set_two_three = set(df_two_in_three[\"twogram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twogram_in_threegram = pd.DataFrame(set_two_three, columns=[\"twogram\"])  # columns=[\"twogram_in_threegram\"]\n",
    "df_twogram_in_threegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twogram_in_threegram_freq = pd.merge(df_twogram_in_threegram, df_shared_select_twogram, how=\"left\", on=\"twogram\")\n",
    "df_twogram_in_threegram_freq.sort_values(by=\"freq_twogram\", ascending=False, inplace=True)\n",
    "df_twogram_in_threegram_freq.rename(columns={\"twogram\":\"twogram_in_threegram\",\"freq_twogram\":\"freq_two_in_three\"}, inplace=True)\n",
    "df_twogram_in_threegram_freq.reset_index(drop=True, inplace=True)\n",
    "df_twogram_in_threegram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twogram_diff = pd.DataFrame(set_shared_twogram.difference(set_two_three), columns=[\"twogram\"])\n",
    "df_twogram_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twogram_diff_freq = pd.merge(df_twogram_diff, df_shared_select_twogram, how=\"left\", on=\"twogram\")\n",
    "df_twogram_diff_freq.sort_values(by=\"freq_twogram\", ascending=False, inplace=True)\n",
    "df_twogram_diff_freq.reset_index(drop=True, inplace=True)\n",
    "df_twogram_diff_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_file[\"twogram\"] = df_twogram_diff_freq[\"twogram\"]\n",
    "df_shared_file[\"freq_twogram\"] = df_twogram_diff_freq[\"freq_twogram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_twogram_process = pd.concat([df_shared_file,df_twogram_in_threegram_freq], axis=1)\n",
    "df_shared_twogram_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_twogram_process.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Result_With_Frequency{file_ext}2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concat Result With Comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_twogram = word_in_wordgroup(df_shared_twogram_process, \"word\", \"twogram\")\n",
    "df_word_order_threegram = word_in_wordgroup(df_shared_twogram_process, \"word\", \"threegram\") \n",
    "df_word_order_fourgram = word_in_wordgroup(df_shared_twogram_process, \"word\", \"fourgram\") \n",
    "df_word_order_fivegram = word_in_wordgroup(df_shared_twogram_process, \"word\", \"fivegram\")\n",
    "df_word_order_sentence = word_in_wordgroup(df_shared_twogram_process, \"word\", \"sentence\")\n",
    "df_word_order_twogram_threegram = word_in_wordgroup(df_shared_twogram_process, \"word\", \"twogram_in_threegram\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_twogram = df_word_order_twogram.groupby([\"word\"])[\"twogram\"].apply(\", \".join).reset_index()   # df_word_order_11.groupby([\"word\"])[f\"twogram_pair_{lang_pair.lower()}\"].transform(lambda x: ','.join(x))\n",
    "df_word_order_join_threegram = df_word_order_threegram.groupby([\"word\"])[\"threegram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_fourgram = df_word_order_fourgram.groupby([\"word\"])[\"fourgram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_fivegram = df_word_order_fivegram.groupby([\"word\"])[\"fivegram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_sentence = df_word_order_sentence.groupby([\"word\"])[\"sentence\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_twogram_threegram = df_word_order_twogram_threegram.groupby([\"word\"])[\"twogram_in_threegram\"].apply(\", \".join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_word_order_join_twogram,df_word_order_join_threegram,df_word_order_join_fourgram,df_word_order_join_fivegram,df_word_order_join_sentence,df_word_order_join_twogram_threegram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_all = reduce(lambda  left,right: pd.merge(left,right, on=['word'], how='outer'), dfs)  # left,right make left to right merge\n",
    "#df_word_order_join_all = reduce(lambda  right,left: pd.merge(left,right, on=['word'], how='outer'), dfs)  # right,left make right to left merge\n",
    "df_word_order_join_all  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option For Word Frequency\n",
    "if shared_word_frequency:\n",
    "    df_word_order_join_all = pd.merge(df_word_order_join_all,df_word_all, how=\"inner\", on=\"word\")\n",
    "    df_word_order_join_all.drop_duplicates(inplace=True)\n",
    "    df_word_order_join_all = df_word_order_join_all.loc[:,[\"word\",\"frequency\",\"twogram\",\"threegram\",\"fourgram\",\"fivegram\",\"sentence\",\"twogram_in_threegram\"]]\n",
    "    df_word_order_join_all.sort_values(by=\"frequency\", inplace=True, ascending=False)\n",
    "    df_word_order_join_all.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "df_word_order_join_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_all.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Join_Result_Without_Frequency{file_ext}2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = glob.glob(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared*{file_ext}2.xlsx\")\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in output_file:\n",
    "    source = k # source directory\n",
    "    destination = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\"\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in output_file:\n",
    "    try:\n",
    "        os.remove(i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefix Suffix Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "lang_pair = \"Italian\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# adding native word to shared word\n",
    "word_start = 0  # 0  # native word start index\n",
    "word_end = 28  # 28  # native word end index\n",
    "\n",
    "# word sample\n",
    "word_sample = True  # True, False\n",
    "word_sample_num = 20\n",
    "\n",
    "# shared word frequency\n",
    "shared_word_frequency = True  # True, False\n",
    "\n",
    "# prefix suffix file\n",
    "prefix_suffix = True  # True, False  # always must be True in this part\n",
    "native_word = False # True for adding native word\n",
    "etymology_word = True  # True for adding etymology word\n",
    "\n",
    "# adding output file extention\n",
    "if (not prefix_suffix) & etymology_word & native_word:\n",
    "    file_ext = \"1\"\n",
    "elif (not prefix_suffix) & etymology_word & (not native_word):\n",
    "    file_ext = \"2\"\n",
    "elif prefix_suffix & etymology_word & native_word:\n",
    "    file_ext = \"3\"\n",
    "elif prefix_suffix & etymology_word & (not native_word):\n",
    "    file_ext = \"4\"\n",
    "elif prefix_suffix & (not etymology_word) & native_word:\n",
    "    file_ext = \"5\"\n",
    "elif (not prefix_suffix) & (not etymology_word) & native_word:\n",
    "    file_ext = \"6\"\n",
    "else:\n",
    "    file_ext = \"7\"              \n",
    "# 1 => for native word and etymology word without prefix suffix. \n",
    "# 2 => for only etymology word without prefix suffix. \n",
    "# 3 => for native word and etymology word with prefix suffix. prefix_suffix, native_word and etymology_word must be True. \n",
    "# 4 => for only etymology word with prefix suffix.\n",
    "# 5 => for only native word with prefix suffix.\n",
    "# 6 => for only native word without prefix suffix.\n",
    "print(f\"{file_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_in_wordgroup(df, source_column, target_column):\n",
    "\n",
    "    '''word_in_wordgroup(df, \"word\", \"twogram\"):\n",
    "       df is dataframe, source_column and target_column are \n",
    "       dataframe column string name. source_column convert list\n",
    "       values that are in target column.\n",
    "    '''\n",
    "    \n",
    "    df_select = df[[f\"{target_column}\"]].dropna()\n",
    "    df_word_result = pd.DataFrame()\n",
    "    for i in df[f\"{source_column}\"].dropna():\n",
    "        try:\n",
    "            if word_sample:\n",
    "                word_in_word_cluster = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)].head(word_sample_num)  # Option\n",
    "            else:\n",
    "                word_in_word_cluster = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)] \n",
    "        except:\n",
    "            pass        \n",
    "        word_in_word_cluster.insert(0,f\"{source_column}\",i)\n",
    "        df_word_result = pd.concat([df_word_result,word_in_word_cluster], axis=0)\n",
    "    df_word_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_word_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.lower().capitalize()}/Deployment/Data/Word/Word_Merge_Preprocess.xlsx\")\n",
    "df_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_prefix_suffix = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_{word_end}_Word_Prefix_Suffix_Custom_Result_Manuel.xlsx\")\n",
    "df_word_prefix_suffix = df_word_prefix_suffix[[\"search_word\",\"word\"]]\n",
    "df_word_prefix_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ety_prefix_suffix = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Word_Prefix_Suffix_Custom_Result.xlsx\")\n",
    "df_ety_prefix_suffix = df_ety_prefix_suffix[[\"search_word\",\"word\"]]\n",
    "df_ety_prefix_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if file_ext == \"3\":\n",
    "    df_all_word = pd.concat([df_word_prefix_suffix,df_ety_prefix_suffix],axis=0)\n",
    "    df_all_word.drop_duplicates(inplace=True)\n",
    "    df_all_word.reset_index(drop=True, inplace=True)\n",
    "elif file_ext == \"4\":\n",
    "    df_all_word = df_ety_prefix_suffix\n",
    "    df_all_word.drop_duplicates(inplace=True)\n",
    "    df_all_word.reset_index(drop=True, inplace=True)\n",
    "elif file_ext == \"5\":\n",
    "    df_all_word = df_word_prefix_suffix\n",
    "    df_all_word.drop_duplicates(inplace=True)\n",
    "    df_all_word.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    pass\n",
    "    \n",
    "df_all_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_word.search_word.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_word.word.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/4-Shared Word/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Result_With_Frequency{file_ext}.xlsx\")\n",
    "#df_shared_all = pd.read_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Result_With_Frequency.xlsx\")\n",
    "df_shared_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_twogram = word_in_wordgroup(df_shared_all, \"word\", \"twogram\")\n",
    "df_word_order_threegram = word_in_wordgroup(df_shared_all, \"word\", \"threegram\") \n",
    "df_word_order_fourgram = word_in_wordgroup(df_shared_all, \"word\", \"fourgram\") \n",
    "df_word_order_fivegram = word_in_wordgroup(df_shared_all, \"word\", \"fivegram\")\n",
    "df_word_order_sentence = word_in_wordgroup(df_shared_all, \"word\", \"sentence\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_twogram = pd.merge(df_word_order_twogram,df_all_word, how=\"inner\", on=\"word\")\n",
    "df_word_order_threegram = pd.merge(df_word_order_threegram,df_all_word, how=\"inner\", on=\"word\")\n",
    "df_word_order_fourgram = pd.merge(df_word_order_fourgram,df_all_word, how=\"inner\", on=\"word\")\n",
    "df_word_order_fivegram = pd.merge(df_word_order_fivegram,df_all_word, how=\"inner\", on=\"word\")\n",
    "df_word_order_sentence = pd.merge(df_word_order_sentence,df_all_word, how=\"inner\", on=\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_twogram = df_word_order_twogram.groupby([\"search_word\"])[\"twogram\"].apply(\", \".join).reset_index()   # df_word_order_11.groupby([\"word\"])[f\"twogram_pair_{lang_pair.lower()}\"].transform(lambda x: ','.join(x))\n",
    "df_word_order_join_threegram = df_word_order_threegram.groupby([\"search_word\"])[\"threegram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_fourgram = df_word_order_fourgram.groupby([\"search_word\"])[\"fourgram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_fivegram = df_word_order_fivegram.groupby([\"search_word\"])[\"fivegram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_sentence = df_word_order_sentence.groupby([\"search_word\"])[\"sentence\"].apply(\", \".join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_word_order_join_twogram,df_word_order_join_threegram,df_word_order_join_fourgram,df_word_order_join_fivegram,df_word_order_join_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_all = reduce(lambda  left,right: pd.merge(left,right, on=['search_word'], how='outer'), dfs)  # left,right make left to right merge\n",
    "#df_word_order_join_all = reduce(lambda  right,left: pd.merge(left,right, on=['word'], how='outer'), dfs)  # right,left make right to left merge\n",
    "df_word_order_join_all.rename(columns={\"search_word\":\"word\"}, inplace=True)\n",
    "df_word_order_join_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option For Word Frequency\n",
    "if shared_word_frequency:\n",
    "    df_word_order_join_all = pd.merge(df_word_order_join_all,df_word_all, how=\"left\", on=\"word\")\n",
    "    df_word_order_join_all.drop_duplicates(inplace=True)\n",
    "    df_word_order_join_all = df_word_order_join_all.loc[:,[\"word\",\"frequency\",\"twogram\",\"threegram\",\"fourgram\",\"fivegram\",\"sentence\"]]\n",
    "    df_word_order_join_all.sort_values(by=\"frequency\", inplace=True, ascending=False)\n",
    "    df_word_order_join_all.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "df_word_order_join_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_all.word.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_all.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Join_Result_Without_Frequency{file_ext}3.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file2 = glob.glob(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared*{file_ext}3.xlsx\")\n",
    "output_file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in output_file2:\n",
    "    source = l # source directory\n",
    "    destination = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\"\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in output_file2:\n",
    "    try:\n",
    "        os.remove(j)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefix Suffix Shared File Word Result Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## language pair (same previous part parameter)\n",
    "#lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "#lang_pair = \"French\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# word sample\n",
    "word_sample_num = 20\n",
    "\n",
    "print(f\"{file_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_strip_func(x):\n",
    "    try:\n",
    "        var_low = x.lower()\n",
    "        var_out = var_low.strip()\n",
    "    except:\n",
    "        var_out = x\n",
    "    return var_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_process_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Join_Result_Without_Frequency{file_ext}3.xlsx\")\n",
    "df_shared_process_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.lower().capitalize()}/Deployment/Data/Word/Word_Merge_Preprocess.xlsx\")\n",
    "df_word_all = df_word_all.loc[:,[\"word\",\"frequency\"]]\n",
    "df_word_all[\"word\"] = df_word_all[\"word\"].apply(lambda x: lower_strip_func(x))\n",
    "df_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twogram_all = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Twogram_Merge.csv\")\n",
    "df_twogram_all = df_twogram_all.loc[:,[\"twogram\",\"frequency\"]]\n",
    "df_twogram_all[\"twogram\"] = df_twogram_all[\"twogram\"].apply(lambda x: lower_strip_func(x))\n",
    "df_twogram_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_threegram_all = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Threegram_Merge.csv\")\n",
    "df_threegram_all = df_threegram_all.loc[:,[\"threegram\",\"frequency\"]]\n",
    "df_threegram_all[\"threegram\"] = df_threegram_all[\"threegram\"].apply(lambda x: lower_strip_func(x))\n",
    "df_threegram_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fourgram_all = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Fourgram_Merge.csv\")\n",
    "df_fourgram_all = df_fourgram_all.loc[:,[\"fourgram\",\"frequency\"]]\n",
    "df_fourgram_all[\"fourgram\"] = df_fourgram_all[\"fourgram\"].apply(lambda x: lower_strip_func(x))\n",
    "df_fourgram_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fivegram_all = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Fivegram_Merge.csv\")\n",
    "df_fivegram_all = df_fivegram_all.loc[:,[\"fivegram\",\"frequency\"]]\n",
    "df_fivegram_all[\"fivegram\"] = df_fivegram_all[\"fivegram\"].apply(lambda x: lower_strip_func(x))\n",
    "df_fivegram_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_all = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/Sentence/Merge/Sentence_Merge.csv\")\n",
    "df_sentence_all = df_sentence_all.loc[:,[\"sentence\",\"frequency\"]]\n",
    "df_sentence_all[\"sentence\"] = df_sentence_all[\"sentence\"].apply(lambda x: lower_strip_func(x))\n",
    "df_sentence_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_shared_process_all)):\n",
    "    # column result\n",
    "    try:\n",
    "        # column result\n",
    "        df_two_var = pd.DataFrame(df_shared_process_all.loc[i,\"twogram\"].split(\", \"), columns=[\"twogram\"])\n",
    "        # merge with all\n",
    "        df_two_var_merge = pd.merge(df_two_var, df_twogram_all, how=\"left\", on=\"twogram\")\n",
    "        df_two_var_merge.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "        df_two_var_merge_select = df_two_var_merge.head(word_sample_num)\n",
    "        # to list value\n",
    "        df_two_var_list = df_two_var_merge_select[\"twogram\"].to_list()\n",
    "        # list join\n",
    "        df_two_var_list_join = \", \".join(df_two_var_list)\n",
    "        # change value\n",
    "        df_shared_process_all.loc[i,\"twogram\"] = df_two_var_list_join\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        # column result\n",
    "        df_three_var = pd.DataFrame(df_shared_process_all.loc[i,\"threegram\"].split(\", \"), columns=[\"threegram\"])\n",
    "        # merge with all\n",
    "        df_three_var_merge = pd.merge(df_three_var, df_threegram_all, how=\"left\", on=\"threegram\")\n",
    "        df_three_var_merge.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "        df_three_var_merge_select = df_three_var_merge.head(word_sample_num)\n",
    "        # to list value\n",
    "        df_three_var_list = df_three_var_merge_select[\"threegram\"].to_list()\n",
    "        # list join\n",
    "        df_three_var_list_join = \", \".join(df_three_var_list)\n",
    "        # change value\n",
    "        df_shared_process_all.loc[i,\"threegram\"] = df_three_var_list_join\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        # column result\n",
    "        df_four_var = pd.DataFrame(df_shared_process_all.loc[i,\"fourgram\"].split(\", \"), columns=[\"fourgram\"])\n",
    "        # merge with all\n",
    "        df_four_var_merge = pd.merge(df_four_var, df_fourgram_all, how=\"left\", on=\"fourgram\")\n",
    "        df_four_var_merge.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "        df_four_var_merge_select = df_four_var_merge.head(word_sample_num)\n",
    "        # to list value\n",
    "        df_four_var_list = df_four_var_merge_select[\"fourgram\"].to_list()\n",
    "        # list join\n",
    "        df_four_var_list_join = \", \".join(df_four_var_list)\n",
    "        # change value\n",
    "        df_shared_process_all.loc[i,\"fourgram\"] = df_four_var_list_join\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        # column result\n",
    "        df_five_var = pd.DataFrame(df_shared_process_all.loc[i,\"fivegram\"].split(\", \"), columns=[\"fivegram\"])\n",
    "        # merge with all\n",
    "        df_five_var_merge = pd.merge(df_five_var, df_fivegram_all, how=\"left\", on=\"fivegram\")\n",
    "        df_five_var_merge.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "        df_five_var_merge_select = df_five_var_merge.head(word_sample_num)\n",
    "        # to list value\n",
    "        df_five_var_list = df_five_var_merge_select[\"fivegram\"].to_list()\n",
    "        # list join\n",
    "        df_five_var_list_join = \", \".join(df_five_var_list)\n",
    "        # change value\n",
    "        df_shared_process_all.loc[i,\"fivegram\"] = df_five_var_list_join\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        # column result\n",
    "        df_sentence_var = pd.DataFrame(df_shared_process_all.loc[i,\"sentence\"].split(\", \"), columns=[\"sentence\"])\n",
    "        # merge with all\n",
    "        df_sentence_var_merge = pd.merge(df_sentence_var, df_sentence_all, how=\"left\", on=\"sentence\")\n",
    "        df_sentence_var_merge.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "        df_sentence_var_merge_select = df_sentence_var_merge.head(word_sample_num)\n",
    "        # to list value\n",
    "        df_sentence_var_list = df_sentence_var_merge_select[\"sentence\"].to_list()\n",
    "        # list join\n",
    "        df_sentence_var_list_join = \", \".join(df_sentence_var_list)\n",
    "        # change value\n",
    "        df_shared_process_all.loc[i,\"sentence\"] = df_sentence_var_list_join\n",
    "    except:\n",
    "        pass      \n",
    "\n",
    "df_shared_process_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_shared_process_all.sort_values(by=\"frequency\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_process_all.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Join_Select_Result_Without_Frequency{file_ext}4.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file3 = glob.glob(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared*_Select_*{file_ext}4.xlsx\")\n",
    "output_file3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in output_file3:\n",
    "    source = l # source directory\n",
    "    destination = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\"\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in output_file3:\n",
    "    try:\n",
    "        os.remove(j)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "lang_pair = \"English\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/Deploy Result Manuel\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_result(df,column_list): # df is dataframe, column_list is list value\n",
    "    '''\n",
    "    word_count_bool(df, column_list): df columns word count for word frequency\\n\n",
    "    df is dataframe, column_list is list value\\n\n",
    "    word_count_bool(df, [\"word\",\"twogram\"]):\n",
    "    '''\n",
    "    list_all = []\n",
    "    for i in df.loc[:,[x for x in column_list]].columns:\n",
    "        var_list = df[f\"{i}\"].dropna().tolist()\n",
    "        for j in var_list:\n",
    "            list_all.append(j)\n",
    "    text = \" \".join(list_all)\n",
    "    word_list = re.findall(r\"\\w+\",text, re.UNICODE)\n",
    "    df_word_list = pd.DataFrame(word_list, columns=[\"word\"])\n",
    "    #df_word_list.rename(columns={0:\"word\"}, inplace=True)\n",
    "    df_word_count = pd.DataFrame(df_word_list.value_counts())\n",
    "    df_word_count.reset_index(inplace=True)\n",
    "    df_word_count.rename(columns={0:\"word_count\"}, inplace=True)\n",
    "    \n",
    "    return  df_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid = pd.read_excel(\"Turkish English manual selected 2 gram hybrids 3.xlsx\", sheet_name=\"2 gram hybrid\")\n",
    "df_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid_count = word_count_result(df_hybrid, [\"twogram_pair1\",\"twogram_pair2\",\"twogram_pair3\",\"twogram_pair4\"])\n",
    "df_hybrid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid_count_merge = pd.merge(df_hybrid,df_hybrid_count,how=\"left\",on=\"word\")\n",
    "df_hybrid_count_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid_count_merge2 = pd.merge(df_hybrid,df_hybrid_count,how=\"outer\",on=\"word\")\n",
    "df_hybrid_count_merge2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(f\"{lang_folder}_{lang_pair}_Hybrid_Word_Count.xlsx\", engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid_count_merge.to_excel(writer, sheet_name='28_Hybrid_Word_Count', index=False)\n",
    "df_hybrid_count_merge2.to_excel(writer, sheet_name='All_Hybrid_Word_Count', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.read_excel(\"Turkish English manual selected 2 gram hybrids 3.xlsx\", sheet_name=\"2 gram target\")\n",
    "df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_count = word_count_result(df_target, [\"twogram_1\",\"twogram_2\",\"twogram_3\",\"twogram_4\"])\n",
    "df_target_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_count_merge = pd.merge(df_target,df_target_count,how=\"left\",on=\"word\")\n",
    "df_target_count_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_count_merge2 = pd.merge(df_target,df_target_count,how=\"outer\",on=\"word\")\n",
    "df_target_count_merge2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer2 = pd.ExcelWriter(f\"{lang_folder}_{lang_pair}_Target_Word_Count.xlsx\", engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_count_merge.to_excel(writer2, sheet_name='28_Target_Word_Count', index=False)\n",
    "df_target_count_merge2.to_excel(writer2, sheet_name='All_Target_Word_Count', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer2.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Target Hybrid Word Count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_word = pd.concat([df_target_count, df_hybrid_count], axis=0)\n",
    "df_all_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_word.groupby(\"word\")[[\"word_count\"]].sum().reset_index(inplace=True)\n",
    "df_all_word.sort_values(by=\"word_count\", ascending=False, inplace=True)\n",
    "df_all_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_word.to_excel(f\"{lang_folder}_{lang_pair}_Target_Hybrid_Word_Count.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file4 = glob.glob(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_*_Word_Count.xlsx\")\n",
    "output_file4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in output_file4:\n",
    "    source = o # source directory\n",
    "    destination = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\"\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in output_file4:\n",
    "    try:\n",
    "        os.remove(p)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Youtube Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "lang_pair = \"English\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# parameter\n",
    "sheets = \"2 gram target\"  # 2 gram target, 2 gram hybrids\n",
    "time_shift = 0.3\n",
    "sample_num = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>sentence</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:02:51.948</td>\n",
       "      <td>00:02:58.829</td>\n",
       "      <td>zgr bunlar normalde kamyon daha byk arala...</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:03:00.956</td>\n",
       "      <td>00:03:04.236</td>\n",
       "      <td>burcu arka taraf balamak kolay olmayacak</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:03:13.434</td>\n",
       "      <td>00:03:16.327</td>\n",
       "      <td>zgr arabaya yarm tur attracam</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:03:17.235</td>\n",
       "      <td>00:03:21.338</td>\n",
       "      <td>burcu biraz daha devam et devam et tamam oldu</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:03:27.806</td>\n",
       "      <td>00:03:33.383</td>\n",
       "      <td>burcu imdilik iki tekere takacaz ama kar ka...</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036932</th>\n",
       "      <td>00:07:51.970</td>\n",
       "      <td>00:07:52.470</td>\n",
       "      <td>umarz ki bu byk ve gl teknoloji yanl e...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036933</th>\n",
       "      <td>00:07:52.470</td>\n",
       "      <td>00:08:02.304</td>\n",
       "      <td>daha faydal ve zgn kurumlarda herkesin eit...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036934</th>\n",
       "      <td>00:08:02.498</td>\n",
       "      <td>00:08:04.178</td>\n",
       "      <td>i zlediiniz iin teekkrler</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036935</th>\n",
       "      <td>00:08:04.178</td>\n",
       "      <td>00:08:08.089</td>\n",
       "      <td>yararlandm kaynaklar aklamada link olara...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036936</th>\n",
       "      <td>00:08:08.089</td>\n",
       "      <td>00:08:11.770</td>\n",
       "      <td>i leri dzey okuma ve aratrma yapmak isteyen...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3036937 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           start_time      end_time  \\\n",
       "0        00:02:51.948  00:02:58.829   \n",
       "1        00:03:00.956  00:03:04.236   \n",
       "2        00:03:13.434  00:03:16.327   \n",
       "3        00:03:17.235  00:03:21.338   \n",
       "4        00:03:27.806  00:03:33.383   \n",
       "...               ...           ...   \n",
       "3036932  00:07:51.970  00:07:52.470   \n",
       "3036933  00:07:52.470  00:08:02.304   \n",
       "3036934  00:08:02.498  00:08:04.178   \n",
       "3036935  00:08:04.178  00:08:08.089   \n",
       "3036936  00:08:08.089  00:08:11.770   \n",
       "\n",
       "                                                  sentence     video_id  \n",
       "0        zgr bunlar normalde kamyon daha byk arala...  8V9tq1pe8eI  \n",
       "1               burcu arka taraf balamak kolay olmayacak  8V9tq1pe8eI  \n",
       "2                      zgr arabaya yarm tur attracam  8V9tq1pe8eI  \n",
       "3            burcu biraz daha devam et devam et tamam oldu  8V9tq1pe8eI  \n",
       "4        burcu imdilik iki tekere takacaz ama kar ka...  8V9tq1pe8eI  \n",
       "...                                                    ...          ...  \n",
       "3036932  umarz ki bu byk ve gl teknoloji yanl e...  YFFJ5FyZj4Q  \n",
       "3036933  daha faydal ve zgn kurumlarda herkesin eit...  YFFJ5FyZj4Q  \n",
       "3036934                      i zlediiniz iin teekkrler  YFFJ5FyZj4Q  \n",
       "3036935  yararlandm kaynaklar aklamada link olara...  YFFJ5FyZj4Q  \n",
       "3036936  i leri dzey okuma ve aratrma yapmak isteyen...  YFFJ5FyZj4Q  \n",
       "\n",
       "[3036937 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_youtube_sentence = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Youtube/Result/{lang_folder.capitalize()}/Sentence Clean Merge/Clean_Youtube_Sentence_Merge_Result.csv\")\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = pd.to_timedelta(df_youtube_sentence['start_time']) # data type converted timedelta for second \n",
    "df_youtube_sentence['end_time'] = pd.to_timedelta(df_youtube_sentence['end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>sentence</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171.948</td>\n",
       "      <td>178.829</td>\n",
       "      <td>zgr bunlar normalde kamyon daha byk arala...</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180.956</td>\n",
       "      <td>184.236</td>\n",
       "      <td>burcu arka taraf balamak kolay olmayacak</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>193.434</td>\n",
       "      <td>196.327</td>\n",
       "      <td>zgr arabaya yarm tur attracam</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>197.235</td>\n",
       "      <td>201.338</td>\n",
       "      <td>burcu biraz daha devam et devam et tamam oldu</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>207.806</td>\n",
       "      <td>213.383</td>\n",
       "      <td>burcu imdilik iki tekere takacaz ama kar ka...</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036932</th>\n",
       "      <td>471.970</td>\n",
       "      <td>472.470</td>\n",
       "      <td>umarz ki bu byk ve gl teknoloji yanl e...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036933</th>\n",
       "      <td>472.470</td>\n",
       "      <td>482.304</td>\n",
       "      <td>daha faydal ve zgn kurumlarda herkesin eit...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036934</th>\n",
       "      <td>482.498</td>\n",
       "      <td>484.178</td>\n",
       "      <td>i zlediiniz iin teekkrler</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036935</th>\n",
       "      <td>484.178</td>\n",
       "      <td>488.089</td>\n",
       "      <td>yararlandm kaynaklar aklamada link olara...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036936</th>\n",
       "      <td>488.089</td>\n",
       "      <td>491.770</td>\n",
       "      <td>i leri dzey okuma ve aratrma yapmak isteyen...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3036937 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         start_time  end_time  \\\n",
       "0           171.948   178.829   \n",
       "1           180.956   184.236   \n",
       "2           193.434   196.327   \n",
       "3           197.235   201.338   \n",
       "4           207.806   213.383   \n",
       "...             ...       ...   \n",
       "3036932     471.970   472.470   \n",
       "3036933     472.470   482.304   \n",
       "3036934     482.498   484.178   \n",
       "3036935     484.178   488.089   \n",
       "3036936     488.089   491.770   \n",
       "\n",
       "                                                  sentence     video_id  \n",
       "0        zgr bunlar normalde kamyon daha byk arala...  8V9tq1pe8eI  \n",
       "1               burcu arka taraf balamak kolay olmayacak  8V9tq1pe8eI  \n",
       "2                      zgr arabaya yarm tur attracam  8V9tq1pe8eI  \n",
       "3            burcu biraz daha devam et devam et tamam oldu  8V9tq1pe8eI  \n",
       "4        burcu imdilik iki tekere takacaz ama kar ka...  8V9tq1pe8eI  \n",
       "...                                                    ...          ...  \n",
       "3036932  umarz ki bu byk ve gl teknoloji yanl e...  YFFJ5FyZj4Q  \n",
       "3036933  daha faydal ve zgn kurumlarda herkesin eit...  YFFJ5FyZj4Q  \n",
       "3036934                      i zlediiniz iin teekkrler  YFFJ5FyZj4Q  \n",
       "3036935  yararlandm kaynaklar aklamada link olara...  YFFJ5FyZj4Q  \n",
       "3036936  i leri dzey okuma ve aratrma yapmak isteyen...  YFFJ5FyZj4Q  \n",
       "\n",
       "[3036937 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_youtube_sentence['start_time'] = df_youtube_sentence['start_time'].apply(lambda x: x.total_seconds()) # convert seconds\n",
    "df_youtube_sentence['end_time'] = df_youtube_sentence['end_time'].apply(lambda x: x.total_seconds())\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>twogram_1</th>\n",
       "      <th>twogram_2</th>\n",
       "      <th>twogram_3</th>\n",
       "      <th>twogram_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bir</td>\n",
       "      <td>bir ey</td>\n",
       "      <td>bir daha</td>\n",
       "      <td>bir de</td>\n",
       "      <td>bir ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bu</td>\n",
       "      <td>bu kadar</td>\n",
       "      <td>bu ne</td>\n",
       "      <td>bu ok</td>\n",
       "      <td>bu deil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ne</td>\n",
       "      <td>ne kadar</td>\n",
       "      <td>ne iin</td>\n",
       "      <td>bana ne</td>\n",
       "      <td>ne gibi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ve</td>\n",
       "      <td>ve de</td>\n",
       "      <td>ve evet</td>\n",
       "      <td>ve ne</td>\n",
       "      <td>ve sen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iin</td>\n",
       "      <td>ne iin</td>\n",
       "      <td>benim iin</td>\n",
       "      <td>senin iin</td>\n",
       "      <td>onun iin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mi</td>\n",
       "      <td>deil mi</td>\n",
       "      <td>ben mi</td>\n",
       "      <td>sen mi</td>\n",
       "      <td>evet mi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>o</td>\n",
       "      <td>o kadar</td>\n",
       "      <td>o da</td>\n",
       "      <td>o deil</td>\n",
       "      <td>evet o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ben</td>\n",
       "      <td>ben de</td>\n",
       "      <td>ben deil</td>\n",
       "      <td>hayr ben</td>\n",
       "      <td>ben mi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>de</td>\n",
       "      <td>sen de</td>\n",
       "      <td>bir de</td>\n",
       "      <td>beni de</td>\n",
       "      <td>beni de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ok</td>\n",
       "      <td>bu ok</td>\n",
       "      <td>daha ok</td>\n",
       "      <td>ok var</td>\n",
       "      <td>ok ey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ama</td>\n",
       "      <td>ama bu</td>\n",
       "      <td>evet ama</td>\n",
       "      <td>hayr ama</td>\n",
       "      <td>var ama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>var</td>\n",
       "      <td>var m</td>\n",
       "      <td>ne var</td>\n",
       "      <td>daha var</td>\n",
       "      <td>evet var</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>evet</td>\n",
       "      <td>evet ama</td>\n",
       "      <td>evet bu</td>\n",
       "      <td>evet ben</td>\n",
       "      <td>evet o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>m</td>\n",
       "      <td>var m</td>\n",
       "      <td>hayr m</td>\n",
       "      <td>bana m</td>\n",
       "      <td>daha m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>deil</td>\n",
       "      <td>deil mi</td>\n",
       "      <td>bu deil</td>\n",
       "      <td>hayr deil</td>\n",
       "      <td>o deil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>da</td>\n",
       "      <td>bu da</td>\n",
       "      <td>o da</td>\n",
       "      <td>daha da</td>\n",
       "      <td>bana da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ey</td>\n",
       "      <td>bir ey</td>\n",
       "      <td>bu ey</td>\n",
       "      <td>o ey</td>\n",
       "      <td>ey gibi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hayr</td>\n",
       "      <td>hayr ben</td>\n",
       "      <td>hayr m</td>\n",
       "      <td>hayr deil</td>\n",
       "      <td>hayr yok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>daha</td>\n",
       "      <td>bir daha</td>\n",
       "      <td>daha ok</td>\n",
       "      <td>daha var</td>\n",
       "      <td>daha deil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sen</td>\n",
       "      <td>sen de</td>\n",
       "      <td>sen mi</td>\n",
       "      <td>hayr sen</td>\n",
       "      <td>sen deil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kadar</td>\n",
       "      <td>ne kadar</td>\n",
       "      <td>bu kadar</td>\n",
       "      <td>o kadar</td>\n",
       "      <td>bana kadar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bana</td>\n",
       "      <td>bana ne</td>\n",
       "      <td>bana da</td>\n",
       "      <td>bana m</td>\n",
       "      <td>bu bana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>yok</td>\n",
       "      <td>hayr yok</td>\n",
       "      <td>yok gibi</td>\n",
       "      <td>evet yok</td>\n",
       "      <td>o yok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>onu</td>\n",
       "      <td>ve onu</td>\n",
       "      <td>onu da</td>\n",
       "      <td>evet onu</td>\n",
       "      <td>hayr onu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>seni</td>\n",
       "      <td>seni de</td>\n",
       "      <td>evet seni</td>\n",
       "      <td>hayr seni</td>\n",
       "      <td>seni deil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>beni</td>\n",
       "      <td>beni mi</td>\n",
       "      <td>beni de</td>\n",
       "      <td>beni deil</td>\n",
       "      <td>evet beni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bunu</td>\n",
       "      <td>hayr bunu</td>\n",
       "      <td>evet bunu</td>\n",
       "      <td>bunu deil</td>\n",
       "      <td>bunun iin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>gibi</td>\n",
       "      <td>ne gibi</td>\n",
       "      <td>ey gibi</td>\n",
       "      <td>var gibi</td>\n",
       "      <td>yok gibi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word   twogram_1   twogram_2    twogram_3   twogram_4\n",
       "0     bir     bir ey    bir daha       bir de     bir ok\n",
       "1      bu    bu kadar       bu ne       bu ok    bu deil\n",
       "2      ne    ne kadar     ne iin      bana ne     ne gibi\n",
       "3      ve       ve de     ve evet        ve ne      ve sen\n",
       "4    iin     ne iin  benim iin   senin iin   onun iin\n",
       "5      mi    deil mi      ben mi       sen mi     evet mi\n",
       "6       o     o kadar        o da      o deil      evet o\n",
       "7     ben      ben de   ben deil    hayr ben      ben mi\n",
       "8      de      sen de      bir de      beni de     beni de\n",
       "9     ok      bu ok    daha ok      ok var     ok ey\n",
       "10    ama      ama bu    evet ama    hayr ama     var ama\n",
       "11    var      var m      ne var     daha var    evet var\n",
       "12   evet    evet ama     evet bu     evet ben      evet o\n",
       "13     m      var m    hayr m      bana m     daha m\n",
       "14  deil    deil mi    bu deil  hayr deil     o deil\n",
       "15     da       bu da        o da      daha da     bana da\n",
       "16    ey     bir ey      bu ey        o ey    ey gibi\n",
       "17  hayr   hayr ben    hayr m  hayr deil   hayr yok\n",
       "18   daha    bir daha    daha ok     daha var  daha deil\n",
       "19    sen      sen de      sen mi    hayr sen   sen deil\n",
       "20  kadar    ne kadar    bu kadar      o kadar  bana kadar\n",
       "21   bana     bana ne     bana da      bana m     bu bana\n",
       "22    yok   hayr yok    yok gibi     evet yok       o yok\n",
       "23    onu      ve onu      onu da     evet onu   hayr onu\n",
       "24   seni     seni de   evet seni   hayr seni  seni deil\n",
       "25   beni     beni mi     beni de   beni deil   evet beni\n",
       "26   bunu  hayr bunu   evet bunu   bunu deil  bunun iin\n",
       "27   gibi     ne gibi    ey gibi     var gibi    yok gibi"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_group_select = pd.read_excel(\"Turkish English Manual Selected 2 Gram Hybrids.xlsx\", sheet_name= \"2 gram target\")\n",
    "df_word_group_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bir ey',\n",
       " 'bir daha',\n",
       " 'bir de',\n",
       " 'bir ok',\n",
       " 'bu kadar',\n",
       " 'bu ne',\n",
       " 'bu ok',\n",
       " 'bu deil',\n",
       " 'ne kadar',\n",
       " 'ne iin',\n",
       " 'bana ne',\n",
       " 'ne gibi',\n",
       " 've de',\n",
       " 've evet',\n",
       " 've ne',\n",
       " 've sen',\n",
       " 'ne iin',\n",
       " 'benim iin',\n",
       " 'senin iin',\n",
       " 'onun iin',\n",
       " 'deil mi',\n",
       " 'ben mi',\n",
       " 'sen mi',\n",
       " 'evet mi',\n",
       " 'o kadar',\n",
       " 'o da',\n",
       " 'o deil',\n",
       " 'evet o',\n",
       " 'ben de',\n",
       " 'ben deil',\n",
       " 'hayr ben',\n",
       " 'ben mi',\n",
       " 'sen de',\n",
       " 'bir de',\n",
       " 'beni de',\n",
       " 'beni de',\n",
       " 'bu ok',\n",
       " 'daha ok',\n",
       " 'ok var',\n",
       " 'ok ey',\n",
       " 'ama bu',\n",
       " 'evet ama',\n",
       " 'hayr ama',\n",
       " 'var ama',\n",
       " 'var m',\n",
       " 'ne var',\n",
       " 'daha var',\n",
       " 'evet var',\n",
       " 'evet ama',\n",
       " 'evet bu',\n",
       " 'evet ben',\n",
       " 'evet o',\n",
       " 'var m',\n",
       " 'hayr m',\n",
       " 'bana m',\n",
       " 'daha m',\n",
       " 'deil mi',\n",
       " 'bu deil',\n",
       " 'hayr deil',\n",
       " 'o deil',\n",
       " 'bu da',\n",
       " 'o da',\n",
       " 'daha da',\n",
       " 'bana da',\n",
       " 'bir ey',\n",
       " 'bu ey',\n",
       " 'o ey',\n",
       " 'ey gibi',\n",
       " 'hayr ben',\n",
       " 'hayr m',\n",
       " 'hayr deil',\n",
       " 'hayr yok',\n",
       " 'bir daha',\n",
       " 'daha ok',\n",
       " 'daha var',\n",
       " 'daha deil',\n",
       " 'sen de',\n",
       " 'sen mi',\n",
       " 'hayr sen',\n",
       " 'sen deil',\n",
       " 'ne kadar',\n",
       " 'bu kadar',\n",
       " 'o kadar',\n",
       " 'bana kadar',\n",
       " 'bana ne',\n",
       " 'bana da',\n",
       " 'bana m',\n",
       " 'bu bana',\n",
       " 'hayr yok',\n",
       " 'yok gibi',\n",
       " 'evet yok',\n",
       " 'o yok',\n",
       " 've onu',\n",
       " 'onu da',\n",
       " 'evet onu',\n",
       " 'hayr onu',\n",
       " 'seni de',\n",
       " 'evet seni',\n",
       " 'hayr seni',\n",
       " 'seni deil',\n",
       " 'beni mi',\n",
       " 'beni de',\n",
       " 'beni deil',\n",
       " 'evet beni',\n",
       " 'hayr bunu',\n",
       " 'evet bunu',\n",
       " 'bunu deil',\n",
       " 'bunun iin',\n",
       " 'ne gibi',\n",
       " 'ey gibi',\n",
       " 'var gibi',\n",
       " 'yok gibi']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_list = []\n",
    "for i in range(len(df_word_group_select)):\n",
    "    for j in df_word_group_select.columns[1:]:\n",
    "        string = df_word_group_select.loc[i,j]\n",
    "        search_list.append(string.strip())\n",
    "#search_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_group_youtube(df,search_list,target_column, sample_num): \n",
    "    df_search_result = pd.DataFrame()\n",
    "    for j in search_list:\n",
    "        df_result = df[df[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){j}(?:\\s|$)\", na=True)]\n",
    "        df_result.sort_values(\"sentence\",key=lambda x:x.str.len(), inplace=True)\n",
    "        df_select = df_result.head(sample_num)\n",
    "        df_select.insert(0,\"search_string\",j)\n",
    "        df_search_result = pd.concat([df_search_result,df_select], axis=0)\n",
    "    df_search_result.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return df_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_search_result = pd.DataFrame()\n",
    "#for i in range(len(df_english_select)):\n",
    "#    for j in df_english_select.columns[1:]:\n",
    "#        string = df_english_select.loc[i,j]\n",
    "#        df_result = df_youtube_sent[df_youtube_sent.sentence.str.contains(fr\"(?:\\s|^){string}(?:\\s|$)\", na=True)]\n",
    "#        df_result.sort_values(\"sentence\",key=lambda x:x.str.len(), inplace=True)\n",
    "#        df_select = df_result.head(6)\n",
    "#        df_select.insert(0,\"search_string\",string)\n",
    "#        df_search_result = pd.concat([df_search_result,df_select], axis=0)\n",
    "#df_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kurubal/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df_search_result = word_group_youtube(df_youtube_sentence,search_list,\"sentence\",6)\n",
    "df_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_time_loc_list = []\n",
    "#def word_group_time_loc(df):\n",
    "#    for i in range(len(df)):\n",
    "#        word = df.loc[i,\"search_string\"]\n",
    "#        start_time = df.loc[i,\"{start_time\"]\n",
    "#        end_time = df.loc[i,\"end_time\"]\n",
    "#        sentence = df.loc[i,\"sentence\"]\n",
    "#        video_id = df.loc[i,\"video_id\"]\n",
    "#        time_length = end_time-start_time\n",
    "#        sentence_length = len(sentence)\n",
    "#        time_length_ratio = time_length/sentence_length\n",
    "#        loc_list = []\n",
    "#        for j in re.finditer(fr\"(?:\\s|^){word}(?:\\s|$)\", sentence, re.IGNORECASE|re.UNICODE):\n",
    "#            loc_list.append(j)\n",
    "#            start = loc_list[0].start()\n",
    "#            end = loc_list[0].end()\n",
    "#            start_loc = start_time+(start*time_length_ratio)\n",
    "#            end_loc = start_time+(end*time_length_ratio)\n",
    "#        word_time_loc_list.append([word,start_loc,end_loc,sentence,video_id])\n",
    "#    df_word_time_loc = pd.DataFrame(word_time_loc_list, columns=[\"search_string\",\"start_time\",\"end_time\",\"sentence\",\"video_id\"])\n",
    "#\n",
    "#    return df_word_time_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_time_loc_list = []\n",
    "def word_group_time_loc(df, search, start_sent, end_sent, sent, sent_id):\n",
    "    for i in range(len(df)):\n",
    "        word = df.loc[i,f\"{search}\"]\n",
    "        start_time = df.loc[i,f\"{start_sent}\"]\n",
    "        end_time = df.loc[i,f\"{end_sent}\"]\n",
    "        sentence = df.loc[i,f\"{sent}\"]\n",
    "        video_id = df.loc[i,f\"{sent_id}\"]\n",
    "        time_length = end_time-start_time\n",
    "        sentence_length = len(sentence)\n",
    "        time_length_ratio = time_length/sentence_length\n",
    "        loc_list = []\n",
    "        for j in re.finditer(fr\"(?:\\s|^){word}(?:\\s|$)\", sentence, re.IGNORECASE|re.UNICODE):\n",
    "            loc_list.append(j)\n",
    "            start = loc_list[0].start()\n",
    "            end = loc_list[0].end()\n",
    "            start_loc = start_time+(start*time_length_ratio)\n",
    "            end_loc = start_time+(end*time_length_ratio)\n",
    "        word_time_loc_list.append([word,start_loc,end_loc,sentence,video_id])\n",
    "    df_word_time_loc = pd.DataFrame(word_time_loc_list, columns=[f\"{search}\",f\"{start_sent}\",f\"{end_sent}\",f\"{sent}\",f\"{sent_id}\"])\n",
    "\n",
    "    return df_word_time_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_result = word_group_time_loc(df_search_result, \"search_string\", \"start_time\", \"end_time\", \"sentence\", \"video_id\")\n",
    "df_word_group_time_loc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_shift = 0.3\n",
    "df_word_group_time_loc_result.start_time = df_word_group_time_loc_result.start_time.apply(lambda x: (x-time_shift))\n",
    "df_word_group_time_loc_result.end_time = df_word_group_time_loc_result.end_time.apply(lambda x: (x+time_shift))\n",
    "df_word_group_time_loc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_result.start_time = df_word_group_time_loc_result.start_time.apply(lambda x: round(x))\n",
    "df_word_group_time_loc_result.end_time = df_word_group_time_loc_result.end_time.apply(lambda x: round(x))\n",
    "df_word_group_time_loc_result "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "651d507d70892fab0fc6529d935cd476f6e2eb1791525b76da6cc8da34bc0503"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
