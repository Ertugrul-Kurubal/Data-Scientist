{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Word File Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "#import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool, Queue\n",
    "from itertools import islice\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 16\n"
     ]
    }
   ],
   "source": [
    "nprocs = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {nprocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# language pair\n",
    "lang_folder = \"English\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "lang_pair = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# adding native word to shared word\n",
    "word_start = 0  # 0  # native word start index\n",
    "word_end = 28  # 28  # native word end index\n",
    "\n",
    "# shared word frequency\n",
    "shared_word_frequency = True  # True, False\n",
    "\n",
    "# prefix suffix file\n",
    "prefix_suffix = False  # True, False  # always must be False in this part\n",
    "native_word = True # True for adding native word\n",
    "etymology_word = True  # True for adding etymology word\n",
    "\n",
    "# adding output file extention\n",
    "if (not prefix_suffix) & etymology_word & native_word:\n",
    "    file_ext = \"1\"\n",
    "elif (not prefix_suffix) & etymology_word & (not native_word):\n",
    "    file_ext = \"2\"\n",
    "elif prefix_suffix & etymology_word & native_word:\n",
    "    file_ext = \"3\"\n",
    "elif prefix_suffix & etymology_word & (not native_word):\n",
    "    file_ext = \"4\"\n",
    "elif prefix_suffix & (not etymology_word) & native_word:\n",
    "    file_ext = \"5\"\n",
    "elif (not prefix_suffix) & (not etymology_word) & native_word:\n",
    "    file_ext = \"6\"\n",
    "else:\n",
    "    file_ext = \"7\"              \n",
    "# 1 => for native word and etymology word without prefix suffix. \n",
    "# 2 => for only etymology word without prefix suffix. \n",
    "# 3 => for native word and etymology word with prefix suffix. prefix_suffix, native_word and etymology_word must be True. \n",
    "# 4 => for only etymology word with prefix suffix.\n",
    "# 5 => for only native word with prefix suffix.\n",
    "# 6 => for only native word without prefix suffix.\n",
    "print(f\"{file_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twogram In Threegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you</td>\n",
       "      <td>102069964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i</td>\n",
       "      <td>94447074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>77481215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>58281119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>50852895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552149</th>\n",
       "      <td>fruitcocktail</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552150</th>\n",
       "      <td>andthesunlightshining</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552151</th>\n",
       "      <td>upravo</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552152</th>\n",
       "      <td>yagawa</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552153</th>\n",
       "      <td>foxtrotoscar</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>552154 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         word  frequency\n",
       "0                         you  102069964\n",
       "1                           i   94447074\n",
       "2                         the   77481215\n",
       "3                          to   58281119\n",
       "4                          is   50852895\n",
       "...                       ...        ...\n",
       "552149          fruitcocktail          6\n",
       "552150  andthesunlightshining          6\n",
       "552151                 upravo          6\n",
       "552152                 yagawa          6\n",
       "552153           foxtrotoscar          6\n",
       "\n",
       "[552154 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.lower().capitalize()}/Deployment/Data/Word/Word_Merge_Preprocess.xlsx\")\n",
    "df_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>twogram</th>\n",
       "      <th>freq_twogram</th>\n",
       "      <th>threegram</th>\n",
       "      <th>freq_threegram</th>\n",
       "      <th>fourgram</th>\n",
       "      <th>freq_fourgram</th>\n",
       "      <th>fivegram</th>\n",
       "      <th>freq_fivegram</th>\n",
       "      <th>sentence</th>\n",
       "      <th>freq_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you</td>\n",
       "      <td>102069964.0</td>\n",
       "      <td>i am</td>\n",
       "      <td>10494331.0</td>\n",
       "      <td>i do not</td>\n",
       "      <td>3520257</td>\n",
       "      <td>you do not have</td>\n",
       "      <td>160426.0</td>\n",
       "      <td>you do not have to</td>\n",
       "      <td>107450.0</td>\n",
       "      <td>of course</td>\n",
       "      <td>266277.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i</td>\n",
       "      <td>94447074.0</td>\n",
       "      <td>do not</td>\n",
       "      <td>8972296.0</td>\n",
       "      <td>i am not</td>\n",
       "      <td>1262941</td>\n",
       "      <td>i do not have</td>\n",
       "      <td>158788.0</td>\n",
       "      <td>i do not have a</td>\n",
       "      <td>34568.0</td>\n",
       "      <td>what is it</td>\n",
       "      <td>251203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>77481215.0</td>\n",
       "      <td>i do</td>\n",
       "      <td>4179914.0</td>\n",
       "      <td>what do you</td>\n",
       "      <td>1120086</td>\n",
       "      <td>do not have to</td>\n",
       "      <td>150328.0</td>\n",
       "      <td>do not have to do</td>\n",
       "      <td>21572.0</td>\n",
       "      <td>stop it</td>\n",
       "      <td>152280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>58281119.0</td>\n",
       "      <td>i will</td>\n",
       "      <td>3706224.0</td>\n",
       "      <td>you do not</td>\n",
       "      <td>1010676</td>\n",
       "      <td>no i do not</td>\n",
       "      <td>105374.0</td>\n",
       "      <td>i do not have to</td>\n",
       "      <td>21499.0</td>\n",
       "      <td>i am</td>\n",
       "      <td>136931.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>50852895.0</td>\n",
       "      <td>do you</td>\n",
       "      <td>3358227.0</td>\n",
       "      <td>do not you</td>\n",
       "      <td>644488</td>\n",
       "      <td>i do not like</td>\n",
       "      <td>99318.0</td>\n",
       "      <td>i do not like it</td>\n",
       "      <td>18564.0</td>\n",
       "      <td>i do</td>\n",
       "      <td>114801.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206638</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i control radiation</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206639</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the antibiotic i</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206640</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my radar screen</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206641</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cup will not</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206642</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stop on no</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206643 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word    frequency twogram  freq_twogram            threegram  \\\n",
       "0       you  102069964.0    i am    10494331.0             i do not   \n",
       "1         i   94447074.0  do not     8972296.0             i am not   \n",
       "2       the   77481215.0    i do     4179914.0          what do you   \n",
       "3        to   58281119.0  i will     3706224.0           you do not   \n",
       "4        is   50852895.0  do you     3358227.0           do not you   \n",
       "...     ...          ...     ...           ...                  ...   \n",
       "206638  NaN          NaN     NaN           NaN  i control radiation   \n",
       "206639  NaN          NaN     NaN           NaN     the antibiotic i   \n",
       "206640  NaN          NaN     NaN           NaN      my radar screen   \n",
       "206641  NaN          NaN     NaN           NaN         cup will not   \n",
       "206642  NaN          NaN     NaN           NaN           stop on no   \n",
       "\n",
       "        freq_threegram         fourgram  freq_fourgram            fivegram  \\\n",
       "0              3520257  you do not have       160426.0  you do not have to   \n",
       "1              1262941    i do not have       158788.0     i do not have a   \n",
       "2              1120086   do not have to       150328.0   do not have to do   \n",
       "3              1010676      no i do not       105374.0    i do not have to   \n",
       "4               644488    i do not like        99318.0    i do not like it   \n",
       "...                ...              ...            ...                 ...   \n",
       "206638               4              NaN            NaN                 NaN   \n",
       "206639               4              NaN            NaN                 NaN   \n",
       "206640               4              NaN            NaN                 NaN   \n",
       "206641               4              NaN            NaN                 NaN   \n",
       "206642               4              NaN            NaN                 NaN   \n",
       "\n",
       "        freq_fivegram    sentence  freq_sentence  \n",
       "0            107450.0   of course       266277.0  \n",
       "1             34568.0  what is it       251203.0  \n",
       "2             21572.0     stop it       152280.0  \n",
       "3             21499.0        i am       136931.0  \n",
       "4             18564.0        i do       114801.0  \n",
       "...               ...         ...            ...  \n",
       "206638            NaN         NaN            NaN  \n",
       "206639            NaN         NaN            NaN  \n",
       "206640            NaN         NaN            NaN  \n",
       "206641            NaN         NaN            NaN  \n",
       "206642            NaN         NaN            NaN  \n",
       "\n",
       "[206643 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shared_file = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/4-Shared Word/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Result_With_Frequency{file_ext}.xlsx\")\n",
    "#df_shared_file = pd.read_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Result_With_Frequency.xlsx\")\n",
    "df_shared_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_result(df,column_list): # df is dataframe, column_list is list value\n",
    "    '''\n",
    "    word_count_bool(df, column_list): df columns word count for word frequency\\n\n",
    "    df is dataframe, column_list is list value\\n\n",
    "    word_count_bool(df, [\"word\",\"twogram\"]):\n",
    "    '''\n",
    "    list_all = []\n",
    "    for i in df.loc[:,[x for x in column_list]].columns:\n",
    "        var_list = df[f\"{i}\"].dropna().tolist()\n",
    "        for j in var_list:\n",
    "            list_all.append(j)\n",
    "    text = \" \".join(list_all)\n",
    "    word_list = re.findall(r\"\\w+\",text, re.UNICODE)\n",
    "    df_word_list = pd.DataFrame(word_list, columns=[\"word\"])\n",
    "    #df_word_list.rename(columns={0:\"word\"}, inplace=True)\n",
    "    df_word_count = pd.DataFrame(df_word_list.value_counts())\n",
    "    df_word_count.reset_index(inplace=True)\n",
    "    df_word_count.rename(columns={0:\"word_count\"}, inplace=True)\n",
    "    \n",
    "    return  df_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_in_wordgroup(df, list_column, target_column):\n",
    "\n",
    "    '''word_in_wordgroup(df, \"word\", \"twogram\"):\n",
    "       df is dataframe, list_column and target_column are \n",
    "       dataframe column string name. list_column convert list\n",
    "       values that are in target column.\n",
    "    '''\n",
    "    \n",
    "    df_select = df[[f\"{target_column}\"]].dropna()\n",
    "    df_word_result = pd.DataFrame()\n",
    "    for i in df[f\"{list_column}\"].dropna():\n",
    "        try:\n",
    "            #word_in_twogram = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)].head(10)  # Option\n",
    "            word_in_word_cluster = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)].head(100) \n",
    "        except:\n",
    "            pass        \n",
    "        word_in_word_cluster.insert(0,f\"{list_column}\",i)\n",
    "        df_word_result = pd.concat([df_word_result,word_in_word_cluster], axis=0)\n",
    "    df_word_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_word_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_shared_count = word_count_result(df_shared_file,[\"threegram\"])\n",
    "#df_shared_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78359"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shared_file[\"twogram\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twogram</th>\n",
       "      <th>threegram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am</td>\n",
       "      <td>i am not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am</td>\n",
       "      <td>i am a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am</td>\n",
       "      <td>no i am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am</td>\n",
       "      <td>what i am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am</td>\n",
       "      <td>i am the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334913</th>\n",
       "      <td>champion puzzle</td>\n",
       "      <td>a champion puzzle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334914</th>\n",
       "      <td>quality material</td>\n",
       "      <td>was quality material</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334915</th>\n",
       "      <td>quality mask</td>\n",
       "      <td>a quality mask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334916</th>\n",
       "      <td>quality index</td>\n",
       "      <td>quality index is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334917</th>\n",
       "      <td>classical department</td>\n",
       "      <td>the classical department</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>334918 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     twogram                 threegram\n",
       "0                       i am                  i am not\n",
       "1                       i am                    i am a\n",
       "2                       i am                   no i am\n",
       "3                       i am                 what i am\n",
       "4                       i am                  i am the\n",
       "...                      ...                       ...\n",
       "334913       champion puzzle         a champion puzzle\n",
       "334914      quality material      was quality material\n",
       "334915          quality mask            a quality mask\n",
       "334916         quality index          quality index is\n",
       "334917  classical department  the classical department\n",
       "\n",
       "[334918 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_two_in_three = word_in_wordgroup(df_shared_file, \"twogram\", \"threegram\")\n",
    "df_two_in_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60870"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_two_in_three[\"twogram\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twogram</th>\n",
       "      <th>freq_twogram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am</td>\n",
       "      <td>10494331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do not</td>\n",
       "      <td>8972296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i do</td>\n",
       "      <td>4179914.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i will</td>\n",
       "      <td>3706224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do you</td>\n",
       "      <td>3358227.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206638</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206639</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206640</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206641</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206642</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206643 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       twogram  freq_twogram\n",
       "0         i am    10494331.0\n",
       "1       do not     8972296.0\n",
       "2         i do     4179914.0\n",
       "3       i will     3706224.0\n",
       "4       do you     3358227.0\n",
       "...        ...           ...\n",
       "206638     NaN           NaN\n",
       "206639     NaN           NaN\n",
       "206640     NaN           NaN\n",
       "206641     NaN           NaN\n",
       "206642     NaN           NaN\n",
       "\n",
       "[206643 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shared_select_twogram = df_shared_file.loc[:,[\"twogram\",\"freq_twogram\"]]\n",
    "df_shared_select_twogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_shared_twogram = set(df_shared_select_twogram[\"twogram\"])\n",
    "set_two_three = set(df_two_in_three[\"twogram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twogram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>your screen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hologram is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>action august</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>realism do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>corner depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60865</th>\n",
       "      <td>social plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60866</th>\n",
       "      <td>bouquet the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60867</th>\n",
       "      <td>cafe we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60868</th>\n",
       "      <td>training baby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60869</th>\n",
       "      <td>unit hit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60870 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                twogram\n",
       "0           your screen\n",
       "1           hologram is\n",
       "2         action august\n",
       "3            realism do\n",
       "4      corner depressed\n",
       "...                 ...\n",
       "60865       social plan\n",
       "60866       bouquet the\n",
       "60867           cafe we\n",
       "60868     training baby\n",
       "60869          unit hit\n",
       "\n",
       "[60870 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_twogram_in_threegram = pd.DataFrame(set_two_three, columns=[\"twogram\"])  # columns=[\"twogram_in_threegram\"]\n",
    "df_twogram_in_threegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twogram_in_threegram</th>\n",
       "      <th>freq_two_in_three</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am</td>\n",
       "      <td>10494331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do not</td>\n",
       "      <td>8972296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i do</td>\n",
       "      <td>4179914.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i will</td>\n",
       "      <td>3706224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do you</td>\n",
       "      <td>3358227.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60865</th>\n",
       "      <td>lift music</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60866</th>\n",
       "      <td>counter no</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60867</th>\n",
       "      <td>system design</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60868</th>\n",
       "      <td>have champion</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60869</th>\n",
       "      <td>frequency active</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60870 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      twogram_in_threegram  freq_two_in_three\n",
       "0                     i am         10494331.0\n",
       "1                   do not          8972296.0\n",
       "2                     i do          4179914.0\n",
       "3                   i will          3706224.0\n",
       "4                   do you          3358227.0\n",
       "...                    ...                ...\n",
       "60865           lift music                4.0\n",
       "60866           counter no                4.0\n",
       "60867        system design                4.0\n",
       "60868        have champion                4.0\n",
       "60869     frequency active                4.0\n",
       "\n",
       "[60870 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_twogram_in_threegram_freq = pd.merge(df_twogram_in_threegram, df_shared_select_twogram, how=\"left\", on=\"twogram\")\n",
    "df_twogram_in_threegram_freq.sort_values(by=\"freq_twogram\", ascending=False, inplace=True)\n",
    "df_twogram_in_threegram_freq.rename(columns={\"twogram\":\"twogram_in_threegram\",\"freq_twogram\":\"freq_two_in_three\"}, inplace=True)\n",
    "df_twogram_in_threegram_freq.reset_index(drop=True, inplace=True)\n",
    "df_twogram_in_threegram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twogram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anonymous sponsor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>diet chips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>complete protein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>university hall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17485</th>\n",
       "      <td>subsidy to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17486</th>\n",
       "      <td>shot type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17487</th>\n",
       "      <td>captain post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17488</th>\n",
       "      <td>it initiative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17489</th>\n",
       "      <td>miniature control</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17490 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 twogram\n",
       "0                    NaN\n",
       "1      anonymous sponsor\n",
       "2             diet chips\n",
       "3       complete protein\n",
       "4        university hall\n",
       "...                  ...\n",
       "17485         subsidy to\n",
       "17486          shot type\n",
       "17487       captain post\n",
       "17488      it initiative\n",
       "17489  miniature control\n",
       "\n",
       "[17490 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_twogram_diff = pd.DataFrame(set_shared_twogram.difference(set_two_three), columns=[\"twogram\"])\n",
    "df_twogram_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twogram</th>\n",
       "      <th>freq_twogram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>media group</td>\n",
       "      <td>4604.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello sheriff</td>\n",
       "      <td>501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello mark</td>\n",
       "      <td>457.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello princess</td>\n",
       "      <td>433.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello general</td>\n",
       "      <td>398.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145768</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145769</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145770</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145771</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145772</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145773 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               twogram  freq_twogram\n",
       "0          media group        4604.0\n",
       "1        hello sheriff         501.0\n",
       "2           hello mark         457.0\n",
       "3       hello princess         433.0\n",
       "4        hello general         398.0\n",
       "...                ...           ...\n",
       "145768             NaN           NaN\n",
       "145769             NaN           NaN\n",
       "145770             NaN           NaN\n",
       "145771             NaN           NaN\n",
       "145772             NaN           NaN\n",
       "\n",
       "[145773 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_twogram_diff_freq = pd.merge(df_twogram_diff, df_shared_select_twogram, how=\"left\", on=\"twogram\")\n",
    "df_twogram_diff_freq.sort_values(by=\"freq_twogram\", ascending=False, inplace=True)\n",
    "df_twogram_diff_freq.reset_index(drop=True, inplace=True)\n",
    "df_twogram_diff_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_file[\"twogram\"] = df_twogram_diff_freq[\"twogram\"]\n",
    "df_shared_file[\"freq_twogram\"] = df_twogram_diff_freq[\"freq_twogram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>twogram</th>\n",
       "      <th>freq_twogram</th>\n",
       "      <th>threegram</th>\n",
       "      <th>freq_threegram</th>\n",
       "      <th>fourgram</th>\n",
       "      <th>freq_fourgram</th>\n",
       "      <th>fivegram</th>\n",
       "      <th>freq_fivegram</th>\n",
       "      <th>sentence</th>\n",
       "      <th>freq_sentence</th>\n",
       "      <th>twogram_in_threegram</th>\n",
       "      <th>freq_two_in_three</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you</td>\n",
       "      <td>102069964.0</td>\n",
       "      <td>media group</td>\n",
       "      <td>4604.0</td>\n",
       "      <td>i do not</td>\n",
       "      <td>3520257</td>\n",
       "      <td>you do not have</td>\n",
       "      <td>160426.0</td>\n",
       "      <td>you do not have to</td>\n",
       "      <td>107450.0</td>\n",
       "      <td>of course</td>\n",
       "      <td>266277.0</td>\n",
       "      <td>i am</td>\n",
       "      <td>10494331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i</td>\n",
       "      <td>94447074.0</td>\n",
       "      <td>hello sheriff</td>\n",
       "      <td>501.0</td>\n",
       "      <td>i am not</td>\n",
       "      <td>1262941</td>\n",
       "      <td>i do not have</td>\n",
       "      <td>158788.0</td>\n",
       "      <td>i do not have a</td>\n",
       "      <td>34568.0</td>\n",
       "      <td>what is it</td>\n",
       "      <td>251203.0</td>\n",
       "      <td>do not</td>\n",
       "      <td>8972296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>77481215.0</td>\n",
       "      <td>hello mark</td>\n",
       "      <td>457.0</td>\n",
       "      <td>what do you</td>\n",
       "      <td>1120086</td>\n",
       "      <td>do not have to</td>\n",
       "      <td>150328.0</td>\n",
       "      <td>do not have to do</td>\n",
       "      <td>21572.0</td>\n",
       "      <td>stop it</td>\n",
       "      <td>152280.0</td>\n",
       "      <td>i do</td>\n",
       "      <td>4179914.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>58281119.0</td>\n",
       "      <td>hello princess</td>\n",
       "      <td>433.0</td>\n",
       "      <td>you do not</td>\n",
       "      <td>1010676</td>\n",
       "      <td>no i do not</td>\n",
       "      <td>105374.0</td>\n",
       "      <td>i do not have to</td>\n",
       "      <td>21499.0</td>\n",
       "      <td>i am</td>\n",
       "      <td>136931.0</td>\n",
       "      <td>i will</td>\n",
       "      <td>3706224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>50852895.0</td>\n",
       "      <td>hello general</td>\n",
       "      <td>398.0</td>\n",
       "      <td>do not you</td>\n",
       "      <td>644488</td>\n",
       "      <td>i do not like</td>\n",
       "      <td>99318.0</td>\n",
       "      <td>i do not like it</td>\n",
       "      <td>18564.0</td>\n",
       "      <td>i do</td>\n",
       "      <td>114801.0</td>\n",
       "      <td>do you</td>\n",
       "      <td>3358227.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206638</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i control radiation</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206639</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the antibiotic i</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206640</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my radar screen</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206641</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cup will not</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206642</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stop on no</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206643 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word    frequency         twogram  freq_twogram            threegram  \\\n",
       "0       you  102069964.0     media group        4604.0             i do not   \n",
       "1         i   94447074.0   hello sheriff         501.0             i am not   \n",
       "2       the   77481215.0      hello mark         457.0          what do you   \n",
       "3        to   58281119.0  hello princess         433.0           you do not   \n",
       "4        is   50852895.0   hello general         398.0           do not you   \n",
       "...     ...          ...             ...           ...                  ...   \n",
       "206638  NaN          NaN             NaN           NaN  i control radiation   \n",
       "206639  NaN          NaN             NaN           NaN     the antibiotic i   \n",
       "206640  NaN          NaN             NaN           NaN      my radar screen   \n",
       "206641  NaN          NaN             NaN           NaN         cup will not   \n",
       "206642  NaN          NaN             NaN           NaN           stop on no   \n",
       "\n",
       "        freq_threegram         fourgram  freq_fourgram            fivegram  \\\n",
       "0              3520257  you do not have       160426.0  you do not have to   \n",
       "1              1262941    i do not have       158788.0     i do not have a   \n",
       "2              1120086   do not have to       150328.0   do not have to do   \n",
       "3              1010676      no i do not       105374.0    i do not have to   \n",
       "4               644488    i do not like        99318.0    i do not like it   \n",
       "...                ...              ...            ...                 ...   \n",
       "206638               4              NaN            NaN                 NaN   \n",
       "206639               4              NaN            NaN                 NaN   \n",
       "206640               4              NaN            NaN                 NaN   \n",
       "206641               4              NaN            NaN                 NaN   \n",
       "206642               4              NaN            NaN                 NaN   \n",
       "\n",
       "        freq_fivegram    sentence  freq_sentence twogram_in_threegram  \\\n",
       "0            107450.0   of course       266277.0                 i am   \n",
       "1             34568.0  what is it       251203.0               do not   \n",
       "2             21572.0     stop it       152280.0                 i do   \n",
       "3             21499.0        i am       136931.0               i will   \n",
       "4             18564.0        i do       114801.0               do you   \n",
       "...               ...         ...            ...                  ...   \n",
       "206638            NaN         NaN            NaN                  NaN   \n",
       "206639            NaN         NaN            NaN                  NaN   \n",
       "206640            NaN         NaN            NaN                  NaN   \n",
       "206641            NaN         NaN            NaN                  NaN   \n",
       "206642            NaN         NaN            NaN                  NaN   \n",
       "\n",
       "        freq_two_in_three  \n",
       "0              10494331.0  \n",
       "1               8972296.0  \n",
       "2               4179914.0  \n",
       "3               3706224.0  \n",
       "4               3358227.0  \n",
       "...                   ...  \n",
       "206638                NaN  \n",
       "206639                NaN  \n",
       "206640                NaN  \n",
       "206641                NaN  \n",
       "206642                NaN  \n",
       "\n",
       "[206643 rows x 14 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shared_twogram_process = pd.concat([df_shared_file,df_twogram_in_threegram_freq], axis=1)\n",
    "df_shared_twogram_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_twogram_process.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Result_With_Frequency{file_ext}2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concat Result With Comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_twogram = word_in_wordgroup(df_shared_twogram_process, \"word\", \"twogram\")\n",
    "df_word_order_threegram = word_in_wordgroup(df_shared_twogram_process, \"word\", \"threegram\") \n",
    "df_word_order_fourgram = word_in_wordgroup(df_shared_twogram_process, \"word\", \"fourgram\") \n",
    "df_word_order_fivegram = word_in_wordgroup(df_shared_twogram_process, \"word\", \"fivegram\")\n",
    "df_word_order_sentence = word_in_wordgroup(df_shared_twogram_process, \"word\", \"sentence\")\n",
    "df_word_order_twogram_threegram = word_in_wordgroup(df_shared_twogram_process, \"word\", \"twogram_in_threegram\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_twogram = df_word_order_twogram.groupby([\"word\"])[\"twogram\"].apply(\", \".join).reset_index()   # df_word_order_11.groupby([\"word\"])[f\"twogram_pair_{lang_pair.lower()}\"].transform(lambda x: ','.join(x))\n",
    "df_word_order_join_threegram = df_word_order_threegram.groupby([\"word\"])[\"threegram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_fourgram = df_word_order_fourgram.groupby([\"word\"])[\"fourgram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_fivegram = df_word_order_fivegram.groupby([\"word\"])[\"fivegram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_sentence = df_word_order_sentence.groupby([\"word\"])[\"sentence\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_twogram_threegram = df_word_order_twogram_threegram.groupby([\"word\"])[\"twogram_in_threegram\"].apply(\", \".join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_word_order_join_twogram,df_word_order_join_threegram,df_word_order_join_fourgram,df_word_order_join_fivegram,df_word_order_join_sentence,df_word_order_join_twogram_threegram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>twogram</th>\n",
       "      <th>threegram</th>\n",
       "      <th>fourgram</th>\n",
       "      <th>fivegram</th>\n",
       "      <th>sentence</th>\n",
       "      <th>twogram_in_threegram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>agent a, a terrorism, solo a, a tribune, mummy...</td>\n",
       "      <td>i am a, i have a, this is a, you have a, it wa...</td>\n",
       "      <td>i am not a, do not have a, do you have a, this...</td>\n",
       "      <td>i do not have a, you do not have a, we do not ...</td>\n",
       "      <td>a what, not a chance, what a surprise, i am a ...</td>\n",
       "      <td>have a, is a, was a, for a, in a, not a, like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abacus</td>\n",
       "      <td>your abacus, abacus and, abacus of, my abacus</td>\n",
       "      <td>like the abacus, abacus to the, and the abacus...</td>\n",
       "      <td>i like the abacus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>and the abacus, my abacus</td>\n",
       "      <td>the abacus, abacus to, abacus is, abacus you, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abnormal</td>\n",
       "      <td>on abnormal, this abnormal, for abnormal, you ...</td>\n",
       "      <td>i am abnormal, in the abnormal, am i abnormal,...</td>\n",
       "      <td>contact in the abnormal, and you have abnormal...</td>\n",
       "      <td>we have on the abnormal, the reaction to the a...</td>\n",
       "      <td>abnormal psychology, i am abnormal, am i abnor...</td>\n",
       "      <td>the abnormal, no abnormal, abnormal psychology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>absorb</td>\n",
       "      <td>i absorb, absorb shock</td>\n",
       "      <td>to absorb the, will absorb the, and absorb it,...</td>\n",
       "      <td>it and absorb it, will absorb the radiation, t...</td>\n",
       "      <td>that will absorb the radiation, the ocean will...</td>\n",
       "      <td>absorb it, to absorb, i absorb, absorb this in...</td>\n",
       "      <td>to absorb, absorb the, absorb it, will absorb,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>absurd</td>\n",
       "      <td>absurd it, absurd you, what absurd, you absurd...</td>\n",
       "      <td>this is absurd, that is absurd, it is absurd, ...</td>\n",
       "      <td>absurd is not it, absurd your position is, you...</td>\n",
       "      <td>is it not absurd to, absurd name for a cricket...</td>\n",
       "      <td>this is absurd, that is absurd, it is absurd, ...</td>\n",
       "      <td>is absurd, absurd to, the absurd, this absurd,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>provoke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>do not provoke, not provoke me, to provoke me,...</td>\n",
       "      <td>do not provoke me, i do not provoke, do not yo...</td>\n",
       "      <td>you do do not provoke, do not you provoke me, ...</td>\n",
       "      <td>do not provoke me, i do not provoke, do not pr...</td>\n",
       "      <td>to provoke, not provoke, provoke me, you provo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>quadrillion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a quadrillion electron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a quadrillion</td>\n",
       "      <td>a quadrillion, quadrillion electron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>retouch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>do not retouch, will retouch it, to retouch it...</td>\n",
       "      <td>we do not retouch, we will retouch it</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>retouch it, not retouch, will retouch, to retouch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>sympathizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>me a sympathizer, a communist sympathizer, is ...</td>\n",
       "      <td>am a communist sympathizer, brother was a symp...</td>\n",
       "      <td>i am a communist sympathizer, your brother was...</td>\n",
       "      <td>communist sympathizer, a sympathizer, i am a c...</td>\n",
       "      <td>a sympathizer, communist sympathizer, sympathi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>unisex</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a unisex toilet, you have unisex, it was unise...</td>\n",
       "      <td>me it was unisex, this is a unisex</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a unisex, a unisex toilet</td>\n",
       "      <td>a unisex, unisex toilet, have unisex, was unis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word                                            twogram  \\\n",
       "0               a  agent a, a terrorism, solo a, a tribune, mummy...   \n",
       "1          abacus      your abacus, abacus and, abacus of, my abacus   \n",
       "2        abnormal  on abnormal, this abnormal, for abnormal, you ...   \n",
       "3          absorb                             i absorb, absorb shock   \n",
       "4          absurd  absurd it, absurd you, what absurd, you absurd...   \n",
       "...           ...                                                ...   \n",
       "1695      provoke                                                NaN   \n",
       "1696  quadrillion                                                NaN   \n",
       "1697      retouch                                                NaN   \n",
       "1698  sympathizer                                                NaN   \n",
       "1699       unisex                                                NaN   \n",
       "\n",
       "                                              threegram  \\\n",
       "0     i am a, i have a, this is a, you have a, it wa...   \n",
       "1     like the abacus, abacus to the, and the abacus...   \n",
       "2     i am abnormal, in the abnormal, am i abnormal,...   \n",
       "3     to absorb the, will absorb the, and absorb it,...   \n",
       "4     this is absurd, that is absurd, it is absurd, ...   \n",
       "...                                                 ...   \n",
       "1695  do not provoke, not provoke me, to provoke me,...   \n",
       "1696                             a quadrillion electron   \n",
       "1697  do not retouch, will retouch it, to retouch it...   \n",
       "1698  me a sympathizer, a communist sympathizer, is ...   \n",
       "1699  a unisex toilet, you have unisex, it was unise...   \n",
       "\n",
       "                                               fourgram  \\\n",
       "0     i am not a, do not have a, do you have a, this...   \n",
       "1                                     i like the abacus   \n",
       "2     contact in the abnormal, and you have abnormal...   \n",
       "3     it and absorb it, will absorb the radiation, t...   \n",
       "4     absurd is not it, absurd your position is, you...   \n",
       "...                                                 ...   \n",
       "1695  do not provoke me, i do not provoke, do not yo...   \n",
       "1696                                                NaN   \n",
       "1697              we do not retouch, we will retouch it   \n",
       "1698  am a communist sympathizer, brother was a symp...   \n",
       "1699                 me it was unisex, this is a unisex   \n",
       "\n",
       "                                               fivegram  \\\n",
       "0     i do not have a, you do not have a, we do not ...   \n",
       "1                                                   NaN   \n",
       "2     we have on the abnormal, the reaction to the a...   \n",
       "3     that will absorb the radiation, the ocean will...   \n",
       "4     is it not absurd to, absurd name for a cricket...   \n",
       "...                                                 ...   \n",
       "1695  you do do not provoke, do not you provoke me, ...   \n",
       "1696                                                NaN   \n",
       "1697                                                NaN   \n",
       "1698  i am a communist sympathizer, your brother was...   \n",
       "1699                                                NaN   \n",
       "\n",
       "                                               sentence  \\\n",
       "0     a what, not a chance, what a surprise, i am a ...   \n",
       "1                             and the abacus, my abacus   \n",
       "2     abnormal psychology, i am abnormal, am i abnor...   \n",
       "3     absorb it, to absorb, i absorb, absorb this in...   \n",
       "4     this is absurd, that is absurd, it is absurd, ...   \n",
       "...                                                 ...   \n",
       "1695  do not provoke me, i do not provoke, do not pr...   \n",
       "1696                                      a quadrillion   \n",
       "1697                                                NaN   \n",
       "1698  communist sympathizer, a sympathizer, i am a c...   \n",
       "1699                          a unisex, a unisex toilet   \n",
       "\n",
       "                                   twogram_in_threegram  \n",
       "0     have a, is a, was a, for a, in a, not a, like ...  \n",
       "1     the abacus, abacus to, abacus is, abacus you, ...  \n",
       "2     the abnormal, no abnormal, abnormal psychology...  \n",
       "3     to absorb, absorb the, absorb it, will absorb,...  \n",
       "4     is absurd, absurd to, the absurd, this absurd,...  \n",
       "...                                                 ...  \n",
       "1695  to provoke, not provoke, provoke me, you provo...  \n",
       "1696                a quadrillion, quadrillion electron  \n",
       "1697  retouch it, not retouch, will retouch, to retouch  \n",
       "1698  a sympathizer, communist sympathizer, sympathi...  \n",
       "1699  a unisex, unisex toilet, have unisex, was unis...  \n",
       "\n",
       "[1700 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_order_join_all = reduce(lambda  left,right: pd.merge(left,right, on=['word'], how='outer'), dfs)  # left,right make left to right merge\n",
    "#df_word_order_join_all = reduce(lambda  right,left: pd.merge(left,right, on=['word'], how='outer'), dfs)  # right,left make right to left merge\n",
    "df_word_order_join_all  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>twogram</th>\n",
       "      <th>threegram</th>\n",
       "      <th>fourgram</th>\n",
       "      <th>fivegram</th>\n",
       "      <th>sentence</th>\n",
       "      <th>twogram_in_threegram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you</td>\n",
       "      <td>102069964</td>\n",
       "      <td>you korean, specific you, cypher you, you comm...</td>\n",
       "      <td>what do you, you do not, do not you, you have ...</td>\n",
       "      <td>you do not have, do you have a, no you do not,...</td>\n",
       "      <td>you do not have to, you do not have a, you do ...</td>\n",
       "      <td>and you, do you, you do, no you do not, you do...</td>\n",
       "      <td>do you, you have, you do, not you, you will, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i</td>\n",
       "      <td>94447074</td>\n",
       "      <td>anonymous i, i absorb, i contract, i wow, dies...</td>\n",
       "      <td>i do not, i am not, i am a, i have to, i will ...</td>\n",
       "      <td>i do not have, no i do not, i do not like, i a...</td>\n",
       "      <td>i do not have a, i do not have to, i do not li...</td>\n",
       "      <td>i am, i do, i will, i do not, i am not, no i d...</td>\n",
       "      <td>i am, i do, i will, i have, i was, and i, what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>77481215</td>\n",
       "      <td>the idealist, fantastic the, sophisticated the...</td>\n",
       "      <td>this is the, i am the, out of the, it was the,...</td>\n",
       "      <td>i am in the, i am not the, this is not the, do...</td>\n",
       "      <td>i do not have the, i do not like the, i am not...</td>\n",
       "      <td>what the, the what, the police, the baby, the ...</td>\n",
       "      <td>in the, to the, on the, of the, what the, for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>58281119</td>\n",
       "      <td>to booster, to princess, to formula, to capaci...</td>\n",
       "      <td>i have to, you have to, we have to, not have t...</td>\n",
       "      <td>do not have to, you will have to, do i have to...</td>\n",
       "      <td>you do not have to, do not have to do, i do no...</td>\n",
       "      <td>i have to, you do not have to, you have to, to...</td>\n",
       "      <td>have to, to the, to me, to do, to you, you to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>50852895</td>\n",
       "      <td>is simultaneous, is feminine, is bourgeois, tu...</td>\n",
       "      <td>what is it, this is not, this is a, this is th...</td>\n",
       "      <td>this is not a, is that what you, this is not t...</td>\n",
       "      <td>you have to do is, is not that what you, is th...</td>\n",
       "      <td>what is it, what is this, what is that, it is,...</td>\n",
       "      <td>this is, is not, is it, what is, is that, it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>camp</td>\n",
       "      <td>6</td>\n",
       "      <td>captain camp, camp zombie, camp doctor, camp e...</td>\n",
       "      <td>in the camp, to the camp, we will camp, in thi...</td>\n",
       "      <td>this is base camp, me to the camp, the camp wa...</td>\n",
       "      <td>this for a band camp, this is a training camp,...</td>\n",
       "      <td>the camp, base camp, what camp, a camp, in the...</td>\n",
       "      <td>the camp, to camp, base camp, this camp, camp ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>session</td>\n",
       "      <td>6</td>\n",
       "      <td>practice session, detector session, session a,...</td>\n",
       "      <td>is in session, the session is, have a session,...</td>\n",
       "      <td>a group therapy session, congress is in sessio...</td>\n",
       "      <td>the meeting is in session, in a group therapy ...</td>\n",
       "      <td>i am in session, school is in session, this is...</td>\n",
       "      <td>in session, a session, the session, this sessi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>captain</td>\n",
       "      <td>6</td>\n",
       "      <td>captain axis, captain camp, control captain, c...</td>\n",
       "      <td>this is captain, i am captain, is the captain,...</td>\n",
       "      <td>this is the captain, i am the captain, this is...</td>\n",
       "      <td>i am the captain of, captain of the football t...</td>\n",
       "      <td>this is the captain, the captain, no captain, ...</td>\n",
       "      <td>the captain, you captain, captain i, is captai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>have</td>\n",
       "      <td>6</td>\n",
       "      <td>have name, have depressed, have tank, caps hav...</td>\n",
       "      <td>do not have, i have to, you have to, i have a,...</td>\n",
       "      <td>you do not have, i do not have, do not have to...</td>\n",
       "      <td>you do not have to, i do not have a, do not ha...</td>\n",
       "      <td>i have, have you, i have to, you do not have t...</td>\n",
       "      <td>i have, you have, have to, we have, have a, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>point</td>\n",
       "      <td>6</td>\n",
       "      <td>point bar, million point, out point, point cap...</td>\n",
       "      <td>not the point, the point is, to the point, the...</td>\n",
       "      <td>what is the point, you have a point, is not th...</td>\n",
       "      <td>that is not the point, what is the point of, i...</td>\n",
       "      <td>the point is, no point, your point, what is th...</td>\n",
       "      <td>the point, a point, your point, point is, poin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2304 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  frequency                                            twogram  \\\n",
       "0         you  102069964  you korean, specific you, cypher you, you comm...   \n",
       "1           i   94447074  anonymous i, i absorb, i contract, i wow, dies...   \n",
       "2         the   77481215  the idealist, fantastic the, sophisticated the...   \n",
       "3          to   58281119  to booster, to princess, to formula, to capaci...   \n",
       "4          is   50852895  is simultaneous, is feminine, is bourgeois, tu...   \n",
       "...       ...        ...                                                ...   \n",
       "2299     camp          6  captain camp, camp zombie, camp doctor, camp e...   \n",
       "2300  session          6  practice session, detector session, session a,...   \n",
       "2301  captain          6  captain axis, captain camp, control captain, c...   \n",
       "2302     have          6  have name, have depressed, have tank, caps hav...   \n",
       "2303    point          6  point bar, million point, out point, point cap...   \n",
       "\n",
       "                                              threegram  \\\n",
       "0     what do you, you do not, do not you, you have ...   \n",
       "1     i do not, i am not, i am a, i have to, i will ...   \n",
       "2     this is the, i am the, out of the, it was the,...   \n",
       "3     i have to, you have to, we have to, not have t...   \n",
       "4     what is it, this is not, this is a, this is th...   \n",
       "...                                                 ...   \n",
       "2299  in the camp, to the camp, we will camp, in thi...   \n",
       "2300  is in session, the session is, have a session,...   \n",
       "2301  this is captain, i am captain, is the captain,...   \n",
       "2302  do not have, i have to, you have to, i have a,...   \n",
       "2303  not the point, the point is, to the point, the...   \n",
       "\n",
       "                                               fourgram  \\\n",
       "0     you do not have, do you have a, no you do not,...   \n",
       "1     i do not have, no i do not, i do not like, i a...   \n",
       "2     i am in the, i am not the, this is not the, do...   \n",
       "3     do not have to, you will have to, do i have to...   \n",
       "4     this is not a, is that what you, this is not t...   \n",
       "...                                                 ...   \n",
       "2299  this is base camp, me to the camp, the camp wa...   \n",
       "2300  a group therapy session, congress is in sessio...   \n",
       "2301  this is the captain, i am the captain, this is...   \n",
       "2302  you do not have, i do not have, do not have to...   \n",
       "2303  what is the point, you have a point, is not th...   \n",
       "\n",
       "                                               fivegram  \\\n",
       "0     you do not have to, you do not have a, you do ...   \n",
       "1     i do not have a, i do not have to, i do not li...   \n",
       "2     i do not have the, i do not like the, i am not...   \n",
       "3     you do not have to, do not have to do, i do no...   \n",
       "4     you have to do is, is not that what you, is th...   \n",
       "...                                                 ...   \n",
       "2299  this for a band camp, this is a training camp,...   \n",
       "2300  the meeting is in session, in a group therapy ...   \n",
       "2301  i am the captain of, captain of the football t...   \n",
       "2302  you do not have to, i do not have a, do not ha...   \n",
       "2303  that is not the point, what is the point of, i...   \n",
       "\n",
       "                                               sentence  \\\n",
       "0     and you, do you, you do, no you do not, you do...   \n",
       "1     i am, i do, i will, i do not, i am not, no i d...   \n",
       "2     what the, the what, the police, the baby, the ...   \n",
       "3     i have to, you do not have to, you have to, to...   \n",
       "4     what is it, what is this, what is that, it is,...   \n",
       "...                                                 ...   \n",
       "2299  the camp, base camp, what camp, a camp, in the...   \n",
       "2300  i am in session, school is in session, this is...   \n",
       "2301  this is the captain, the captain, no captain, ...   \n",
       "2302  i have, have you, i have to, you do not have t...   \n",
       "2303  the point is, no point, your point, what is th...   \n",
       "\n",
       "                                   twogram_in_threegram  \n",
       "0     do you, you have, you do, not you, you will, f...  \n",
       "1     i am, i do, i will, i have, i was, and i, what...  \n",
       "2     in the, to the, on the, of the, what the, for ...  \n",
       "3     have to, to the, to me, to do, to you, you to,...  \n",
       "4     this is, is not, is it, what is, is that, it i...  \n",
       "...                                                 ...  \n",
       "2299  the camp, to camp, base camp, this camp, camp ...  \n",
       "2300  in session, a session, the session, this sessi...  \n",
       "2301  the captain, you captain, captain i, is captai...  \n",
       "2302  i have, you have, have to, we have, have a, no...  \n",
       "2303  the point, a point, your point, point is, poin...  \n",
       "\n",
       "[2304 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option For Word Frequency\n",
    "if shared_word_frequency:\n",
    "    df_word_order_join_all = pd.merge(df_word_order_join_all,df_word_all, how=\"inner\", on=\"word\")\n",
    "    df_word_order_join_all.drop_duplicates(inplace=True)\n",
    "    df_word_order_join_all = df_word_order_join_all.loc[:,[\"word\",\"frequency\",\"twogram\",\"threegram\",\"fourgram\",\"fivegram\",\"sentence\",\"twogram_in_threegram\"]]\n",
    "    df_word_order_join_all.sort_values(by=\"frequency\", inplace=True, ascending=False)\n",
    "    df_word_order_join_all.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "df_word_order_join_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_all.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Join_Result_Without_Frequency{file_ext}2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English_Turkish_Shared_Result_With_Frequency12.xlsx',\n",
       " 'English_Turkish_Shared_Join_Result_Without_Frequency12.xlsx']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file = glob.glob(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared*{file_ext}2.xlsx\")\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in output_file:\n",
    "    source = k # source directory\n",
    "    destination = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\"\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in output_file:\n",
    "    try:\n",
    "        os.remove(i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefix Suffix Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "lang_pair = \"Italian\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# adding native word to shared word\n",
    "word_start = 0  # 0  # native word start index\n",
    "word_end = 28  # 28  # native word end index\n",
    "\n",
    "# word sample\n",
    "word_sample = True  # True, False\n",
    "word_sample_num = 20\n",
    "\n",
    "# shared word frequency\n",
    "shared_word_frequency = True  # True, False\n",
    "\n",
    "# prefix suffix file\n",
    "prefix_suffix = True  # True, False  # always must be True in this part\n",
    "native_word = False # True for adding native word\n",
    "etymology_word = True  # True for adding etymology word\n",
    "\n",
    "# adding output file extention\n",
    "if (not prefix_suffix) & etymology_word & native_word:\n",
    "    file_ext = \"1\"\n",
    "elif (not prefix_suffix) & etymology_word & (not native_word):\n",
    "    file_ext = \"2\"\n",
    "elif prefix_suffix & etymology_word & native_word:\n",
    "    file_ext = \"3\"\n",
    "elif prefix_suffix & etymology_word & (not native_word):\n",
    "    file_ext = \"4\"\n",
    "elif prefix_suffix & (not etymology_word) & native_word:\n",
    "    file_ext = \"5\"\n",
    "elif (not prefix_suffix) & (not etymology_word) & native_word:\n",
    "    file_ext = \"6\"\n",
    "else:\n",
    "    file_ext = \"7\"              \n",
    "# 1 => for native word and etymology word without prefix suffix. \n",
    "# 2 => for only etymology word without prefix suffix. \n",
    "# 3 => for native word and etymology word with prefix suffix. prefix_suffix, native_word and etymology_word must be True. \n",
    "# 4 => for only etymology word with prefix suffix.\n",
    "# 5 => for only native word with prefix suffix.\n",
    "# 6 => for only native word without prefix suffix.\n",
    "print(f\"{file_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_in_wordgroup(df, source_column, target_column):\n",
    "\n",
    "    '''word_in_wordgroup(df, \"word\", \"twogram\"):\n",
    "       df is dataframe, source_column and target_column are \n",
    "       dataframe column string name. source_column convert list\n",
    "       values that are in target column.\n",
    "    '''\n",
    "    \n",
    "    df_select = df[[f\"{target_column}\"]].dropna()\n",
    "    df_word_result = pd.DataFrame()\n",
    "    for i in df[f\"{source_column}\"].dropna():\n",
    "        try:\n",
    "            if word_sample:\n",
    "                word_in_word_cluster = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)].head(word_sample_num)  # Option\n",
    "            else:\n",
    "                word_in_word_cluster = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)] \n",
    "        except:\n",
    "            pass        \n",
    "        word_in_word_cluster.insert(0,f\"{source_column}\",i)\n",
    "        df_word_result = pd.concat([df_word_result,word_in_word_cluster], axis=0)\n",
    "    df_word_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_word_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.lower().capitalize()}/Deployment/Data/Word/Word_Merge_Preprocess.xlsx\")\n",
    "df_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_prefix_suffix = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_{word_end}_Word_Prefix_Suffix_Custom_Result_Manuel.xlsx\")\n",
    "df_word_prefix_suffix = df_word_prefix_suffix[[\"search_word\",\"word\"]]\n",
    "df_word_prefix_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ety_prefix_suffix = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Word_Prefix_Suffix_Custom_Result.xlsx\")\n",
    "df_ety_prefix_suffix = df_ety_prefix_suffix[[\"search_word\",\"word\"]]\n",
    "df_ety_prefix_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if file_ext == \"3\":\n",
    "    df_all_word = pd.concat([df_word_prefix_suffix,df_ety_prefix_suffix],axis=0)\n",
    "    df_all_word.drop_duplicates(inplace=True)\n",
    "    df_all_word.reset_index(drop=True, inplace=True)\n",
    "elif file_ext == \"4\":\n",
    "    df_all_word = df_ety_prefix_suffix\n",
    "    df_all_word.drop_duplicates(inplace=True)\n",
    "    df_all_word.reset_index(drop=True, inplace=True)\n",
    "elif file_ext == \"5\":\n",
    "    df_all_word = df_word_prefix_suffix\n",
    "    df_all_word.drop_duplicates(inplace=True)\n",
    "    df_all_word.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    pass\n",
    "    \n",
    "df_all_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_word.search_word.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_word.word.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/4-Shared Word/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Result_With_Frequency{file_ext}.xlsx\")\n",
    "#df_shared_all = pd.read_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Result_With_Frequency.xlsx\")\n",
    "df_shared_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_twogram = word_in_wordgroup(df_shared_all, \"word\", \"twogram\")\n",
    "df_word_order_threegram = word_in_wordgroup(df_shared_all, \"word\", \"threegram\") \n",
    "df_word_order_fourgram = word_in_wordgroup(df_shared_all, \"word\", \"fourgram\") \n",
    "df_word_order_fivegram = word_in_wordgroup(df_shared_all, \"word\", \"fivegram\")\n",
    "df_word_order_sentence = word_in_wordgroup(df_shared_all, \"word\", \"sentence\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_twogram = pd.merge(df_word_order_twogram,df_all_word, how=\"inner\", on=\"word\")\n",
    "df_word_order_threegram = pd.merge(df_word_order_threegram,df_all_word, how=\"inner\", on=\"word\")\n",
    "df_word_order_fourgram = pd.merge(df_word_order_fourgram,df_all_word, how=\"inner\", on=\"word\")\n",
    "df_word_order_fivegram = pd.merge(df_word_order_fivegram,df_all_word, how=\"inner\", on=\"word\")\n",
    "df_word_order_sentence = pd.merge(df_word_order_sentence,df_all_word, how=\"inner\", on=\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_twogram = df_word_order_twogram.groupby([\"search_word\"])[\"twogram\"].apply(\", \".join).reset_index()   # df_word_order_11.groupby([\"word\"])[f\"twogram_pair_{lang_pair.lower()}\"].transform(lambda x: ','.join(x))\n",
    "df_word_order_join_threegram = df_word_order_threegram.groupby([\"search_word\"])[\"threegram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_fourgram = df_word_order_fourgram.groupby([\"search_word\"])[\"fourgram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_fivegram = df_word_order_fivegram.groupby([\"search_word\"])[\"fivegram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_sentence = df_word_order_sentence.groupby([\"search_word\"])[\"sentence\"].apply(\", \".join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_word_order_join_twogram,df_word_order_join_threegram,df_word_order_join_fourgram,df_word_order_join_fivegram,df_word_order_join_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_all = reduce(lambda  left,right: pd.merge(left,right, on=['search_word'], how='outer'), dfs)  # left,right make left to right merge\n",
    "#df_word_order_join_all = reduce(lambda  right,left: pd.merge(left,right, on=['word'], how='outer'), dfs)  # right,left make right to left merge\n",
    "df_word_order_join_all.rename(columns={\"search_word\":\"word\"}, inplace=True)\n",
    "df_word_order_join_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option For Word Frequency\n",
    "if shared_word_frequency:\n",
    "    df_word_order_join_all = pd.merge(df_word_order_join_all,df_word_all, how=\"left\", on=\"word\")\n",
    "    df_word_order_join_all.drop_duplicates(inplace=True)\n",
    "    df_word_order_join_all = df_word_order_join_all.loc[:,[\"word\",\"frequency\",\"twogram\",\"threegram\",\"fourgram\",\"fivegram\",\"sentence\"]]\n",
    "    df_word_order_join_all.sort_values(by=\"frequency\", inplace=True, ascending=False)\n",
    "    df_word_order_join_all.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "df_word_order_join_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_all.word.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_all.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Join_Result_Without_Frequency{file_ext}3.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file2 = glob.glob(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared*{file_ext}3.xlsx\")\n",
    "output_file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in output_file2:\n",
    "    source = l # source directory\n",
    "    destination = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\"\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in output_file2:\n",
    "    try:\n",
    "        os.remove(j)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefix Suffix Shared File Word Result Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## language pair (same previous part parameter)\n",
    "#lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "#lang_pair = \"French\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# word sample\n",
    "word_sample_num = 20\n",
    "\n",
    "print(f\"{file_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_strip_func(x):\n",
    "    try:\n",
    "        var_low = x.lower()\n",
    "        var_out = var_low.strip()\n",
    "    except:\n",
    "        var_out = x\n",
    "    return var_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_process_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Join_Result_Without_Frequency{file_ext}3.xlsx\")\n",
    "df_shared_process_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.lower().capitalize()}/Deployment/Data/Word/Word_Merge_Preprocess.xlsx\")\n",
    "df_word_all = df_word_all.loc[:,[\"word\",\"frequency\"]]\n",
    "df_word_all[\"word\"] = df_word_all[\"word\"].apply(lambda x: lower_strip_func(x))\n",
    "df_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twogram_all = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Twogram_Merge.csv\")\n",
    "df_twogram_all = df_twogram_all.loc[:,[\"twogram\",\"frequency\"]]\n",
    "df_twogram_all[\"twogram\"] = df_twogram_all[\"twogram\"].apply(lambda x: lower_strip_func(x))\n",
    "df_twogram_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_threegram_all = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Threegram_Merge.csv\")\n",
    "df_threegram_all = df_threegram_all.loc[:,[\"threegram\",\"frequency\"]]\n",
    "df_threegram_all[\"threegram\"] = df_threegram_all[\"threegram\"].apply(lambda x: lower_strip_func(x))\n",
    "df_threegram_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fourgram_all = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Fourgram_Merge.csv\")\n",
    "df_fourgram_all = df_fourgram_all.loc[:,[\"fourgram\",\"frequency\"]]\n",
    "df_fourgram_all[\"fourgram\"] = df_fourgram_all[\"fourgram\"].apply(lambda x: lower_strip_func(x))\n",
    "df_fourgram_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fivegram_all = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Fivegram_Merge.csv\")\n",
    "df_fivegram_all = df_fivegram_all.loc[:,[\"fivegram\",\"frequency\"]]\n",
    "df_fivegram_all[\"fivegram\"] = df_fivegram_all[\"fivegram\"].apply(lambda x: lower_strip_func(x))\n",
    "df_fivegram_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_all = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/Sentence/Merge/Sentence_Merge.csv\")\n",
    "df_sentence_all = df_sentence_all.loc[:,[\"sentence\",\"frequency\"]]\n",
    "df_sentence_all[\"sentence\"] = df_sentence_all[\"sentence\"].apply(lambda x: lower_strip_func(x))\n",
    "df_sentence_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_shared_process_all)):\n",
    "    # column result\n",
    "    try:\n",
    "        # column result\n",
    "        df_two_var = pd.DataFrame(df_shared_process_all.loc[i,\"twogram\"].split(\", \"), columns=[\"twogram\"])\n",
    "        # merge with all\n",
    "        df_two_var_merge = pd.merge(df_two_var, df_twogram_all, how=\"left\", on=\"twogram\")\n",
    "        df_two_var_merge.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "        df_two_var_merge_select = df_two_var_merge.head(word_sample_num)\n",
    "        # to list value\n",
    "        df_two_var_list = df_two_var_merge_select[\"twogram\"].to_list()\n",
    "        # list join\n",
    "        df_two_var_list_join = \", \".join(df_two_var_list)\n",
    "        # change value\n",
    "        df_shared_process_all.loc[i,\"twogram\"] = df_two_var_list_join\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        # column result\n",
    "        df_three_var = pd.DataFrame(df_shared_process_all.loc[i,\"threegram\"].split(\", \"), columns=[\"threegram\"])\n",
    "        # merge with all\n",
    "        df_three_var_merge = pd.merge(df_three_var, df_threegram_all, how=\"left\", on=\"threegram\")\n",
    "        df_three_var_merge.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "        df_three_var_merge_select = df_three_var_merge.head(word_sample_num)\n",
    "        # to list value\n",
    "        df_three_var_list = df_three_var_merge_select[\"threegram\"].to_list()\n",
    "        # list join\n",
    "        df_three_var_list_join = \", \".join(df_three_var_list)\n",
    "        # change value\n",
    "        df_shared_process_all.loc[i,\"threegram\"] = df_three_var_list_join\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        # column result\n",
    "        df_four_var = pd.DataFrame(df_shared_process_all.loc[i,\"fourgram\"].split(\", \"), columns=[\"fourgram\"])\n",
    "        # merge with all\n",
    "        df_four_var_merge = pd.merge(df_four_var, df_fourgram_all, how=\"left\", on=\"fourgram\")\n",
    "        df_four_var_merge.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "        df_four_var_merge_select = df_four_var_merge.head(word_sample_num)\n",
    "        # to list value\n",
    "        df_four_var_list = df_four_var_merge_select[\"fourgram\"].to_list()\n",
    "        # list join\n",
    "        df_four_var_list_join = \", \".join(df_four_var_list)\n",
    "        # change value\n",
    "        df_shared_process_all.loc[i,\"fourgram\"] = df_four_var_list_join\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        # column result\n",
    "        df_five_var = pd.DataFrame(df_shared_process_all.loc[i,\"fivegram\"].split(\", \"), columns=[\"fivegram\"])\n",
    "        # merge with all\n",
    "        df_five_var_merge = pd.merge(df_five_var, df_fivegram_all, how=\"left\", on=\"fivegram\")\n",
    "        df_five_var_merge.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "        df_five_var_merge_select = df_five_var_merge.head(word_sample_num)\n",
    "        # to list value\n",
    "        df_five_var_list = df_five_var_merge_select[\"fivegram\"].to_list()\n",
    "        # list join\n",
    "        df_five_var_list_join = \", \".join(df_five_var_list)\n",
    "        # change value\n",
    "        df_shared_process_all.loc[i,\"fivegram\"] = df_five_var_list_join\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        # column result\n",
    "        df_sentence_var = pd.DataFrame(df_shared_process_all.loc[i,\"sentence\"].split(\", \"), columns=[\"sentence\"])\n",
    "        # merge with all\n",
    "        df_sentence_var_merge = pd.merge(df_sentence_var, df_sentence_all, how=\"left\", on=\"sentence\")\n",
    "        df_sentence_var_merge.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "        df_sentence_var_merge_select = df_sentence_var_merge.head(word_sample_num)\n",
    "        # to list value\n",
    "        df_sentence_var_list = df_sentence_var_merge_select[\"sentence\"].to_list()\n",
    "        # list join\n",
    "        df_sentence_var_list_join = \", \".join(df_sentence_var_list)\n",
    "        # change value\n",
    "        df_shared_process_all.loc[i,\"sentence\"] = df_sentence_var_list_join\n",
    "    except:\n",
    "        pass      \n",
    "\n",
    "df_shared_process_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_shared_process_all.sort_values(by=\"frequency\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_process_all.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Join_Select_Result_Without_Frequency{file_ext}4.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file3 = glob.glob(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared*_Select_*{file_ext}4.xlsx\")\n",
    "output_file3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in output_file3:\n",
    "    source = l # source directory\n",
    "    destination = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\"\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in output_file3:\n",
    "    try:\n",
    "        os.remove(j)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "lang_pair = \"English\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/Deploy Result Manuel\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_result(df,column_list): # df is dataframe, column_list is list value\n",
    "    '''\n",
    "    word_count_bool(df, column_list): df columns word count for word frequency\\n\n",
    "    df is dataframe, column_list is list value\\n\n",
    "    word_count_bool(df, [\"word\",\"twogram\"]):\n",
    "    '''\n",
    "    list_all = []\n",
    "    for i in df.loc[:,[x for x in column_list]].columns:\n",
    "        var_list = df[f\"{i}\"].dropna().tolist()\n",
    "        for j in var_list:\n",
    "            list_all.append(j)\n",
    "    text = \" \".join(list_all)\n",
    "    word_list = re.findall(r\"\\w+\",text, re.UNICODE)\n",
    "    df_word_list = pd.DataFrame(word_list, columns=[\"word\"])\n",
    "    #df_word_list.rename(columns={0:\"word\"}, inplace=True)\n",
    "    df_word_count = pd.DataFrame(df_word_list.value_counts())\n",
    "    df_word_count.reset_index(inplace=True)\n",
    "    df_word_count.rename(columns={0:\"word_count\"}, inplace=True)\n",
    "    \n",
    "    return  df_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid = pd.read_excel(\"Turkish English manual selected 2 gram hybrids 3.xlsx\", sheet_name=\"2 gram hybrid\")\n",
    "df_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid_count = word_count_result(df_hybrid, [\"twogram_pair1\",\"twogram_pair2\",\"twogram_pair3\",\"twogram_pair4\"])\n",
    "df_hybrid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid_count_merge = pd.merge(df_hybrid,df_hybrid_count,how=\"left\",on=\"word\")\n",
    "df_hybrid_count_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid_count_merge2 = pd.merge(df_hybrid,df_hybrid_count,how=\"outer\",on=\"word\")\n",
    "df_hybrid_count_merge2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(f\"{lang_folder}_{lang_pair}_Hybrid_Word_Count.xlsx\", engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid_count_merge.to_excel(writer, sheet_name='28_Hybrid_Word_Count', index=False)\n",
    "df_hybrid_count_merge2.to_excel(writer, sheet_name='All_Hybrid_Word_Count', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.read_excel(\"Turkish English manual selected 2 gram hybrids 3.xlsx\", sheet_name=\"2 gram target\")\n",
    "df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_count = word_count_result(df_target, [\"twogram_1\",\"twogram_2\",\"twogram_3\",\"twogram_4\"])\n",
    "df_target_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_count_merge = pd.merge(df_target,df_target_count,how=\"left\",on=\"word\")\n",
    "df_target_count_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_count_merge2 = pd.merge(df_target,df_target_count,how=\"outer\",on=\"word\")\n",
    "df_target_count_merge2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer2 = pd.ExcelWriter(f\"{lang_folder}_{lang_pair}_Target_Word_Count.xlsx\", engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_count_merge.to_excel(writer2, sheet_name='28_Target_Word_Count', index=False)\n",
    "df_target_count_merge2.to_excel(writer2, sheet_name='All_Target_Word_Count', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer2.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Target Hybrid Word Count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_word = pd.concat([df_target_count, df_hybrid_count], axis=0)\n",
    "df_all_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_word.groupby(\"word\")[[\"word_count\"]].sum().reset_index(inplace=True)\n",
    "df_all_word.sort_values(by=\"word_count\", ascending=False, inplace=True)\n",
    "df_all_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_word.to_excel(f\"{lang_folder}_{lang_pair}_Target_Hybrid_Word_Count.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file4 = glob.glob(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_*_Word_Count.xlsx\")\n",
    "output_file4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in output_file4:\n",
    "    source = o # source directory\n",
    "    destination = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\"\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in output_file4:\n",
    "    try:\n",
    "        os.remove(p)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Youtube Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "lang_pair = \"English\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# parameter\n",
    "sheets = \"2 gram hybrid\"  # 2 gram target, 2 gram hybrid\n",
    "time_shift = 0.6\n",
    "sample_num = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>sentence</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:02:51.948</td>\n",
       "      <td>00:02:58.829</td>\n",
       "      <td>özgür bunlar normalde kamyon daha büyük araçla...</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:03:00.956</td>\n",
       "      <td>00:03:04.236</td>\n",
       "      <td>burcu arka tarafı bağlamak kolay olmayacak</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:03:13.434</td>\n",
       "      <td>00:03:16.327</td>\n",
       "      <td>özgür arabaya yarım tur attıracağım</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:03:17.235</td>\n",
       "      <td>00:03:21.338</td>\n",
       "      <td>burcu biraz daha devam et devam et tamam oldu</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:03:27.806</td>\n",
       "      <td>00:03:33.383</td>\n",
       "      <td>burcu şimdilik iki tekere takacağız ama kar ka...</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036932</th>\n",
       "      <td>00:07:51.970</td>\n",
       "      <td>00:07:52.470</td>\n",
       "      <td>umarız ki bu büyük ve güçlü teknoloji yanlış e...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036933</th>\n",
       "      <td>00:07:52.470</td>\n",
       "      <td>00:08:02.304</td>\n",
       "      <td>daha faydalı ve özgün kurumlarda herkesin eşit...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036934</th>\n",
       "      <td>00:08:02.498</td>\n",
       "      <td>00:08:04.178</td>\n",
       "      <td>i zlediğiniz için teşekkürler</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036935</th>\n",
       "      <td>00:08:04.178</td>\n",
       "      <td>00:08:08.089</td>\n",
       "      <td>yararlandığım kaynakları açıklamada link olara...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036936</th>\n",
       "      <td>00:08:08.089</td>\n",
       "      <td>00:08:11.770</td>\n",
       "      <td>i leri düzey okuma ve araştırma yapmak isteyen...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3036937 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           start_time      end_time  \\\n",
       "0        00:02:51.948  00:02:58.829   \n",
       "1        00:03:00.956  00:03:04.236   \n",
       "2        00:03:13.434  00:03:16.327   \n",
       "3        00:03:17.235  00:03:21.338   \n",
       "4        00:03:27.806  00:03:33.383   \n",
       "...               ...           ...   \n",
       "3036932  00:07:51.970  00:07:52.470   \n",
       "3036933  00:07:52.470  00:08:02.304   \n",
       "3036934  00:08:02.498  00:08:04.178   \n",
       "3036935  00:08:04.178  00:08:08.089   \n",
       "3036936  00:08:08.089  00:08:11.770   \n",
       "\n",
       "                                                  sentence     video_id  \n",
       "0        özgür bunlar normalde kamyon daha büyük araçla...  8V9tq1pe8eI  \n",
       "1               burcu arka tarafı bağlamak kolay olmayacak  8V9tq1pe8eI  \n",
       "2                      özgür arabaya yarım tur attıracağım  8V9tq1pe8eI  \n",
       "3            burcu biraz daha devam et devam et tamam oldu  8V9tq1pe8eI  \n",
       "4        burcu şimdilik iki tekere takacağız ama kar ka...  8V9tq1pe8eI  \n",
       "...                                                    ...          ...  \n",
       "3036932  umarız ki bu büyük ve güçlü teknoloji yanlış e...  YFFJ5FyZj4Q  \n",
       "3036933  daha faydalı ve özgün kurumlarda herkesin eşit...  YFFJ5FyZj4Q  \n",
       "3036934                      i zlediğiniz için teşekkürler  YFFJ5FyZj4Q  \n",
       "3036935  yararlandığım kaynakları açıklamada link olara...  YFFJ5FyZj4Q  \n",
       "3036936  i leri düzey okuma ve araştırma yapmak isteyen...  YFFJ5FyZj4Q  \n",
       "\n",
       "[3036937 rows x 4 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_youtube_sentence = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Youtube/Result/{lang_folder.capitalize()}/Sentence Clean Merge/Clean_Youtube_Sentence_Merge_Result.csv\")\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = pd.to_timedelta(df_youtube_sentence['start_time']) # data type converted timedelta for second \n",
    "df_youtube_sentence['end_time'] = pd.to_timedelta(df_youtube_sentence['end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>sentence</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171.948</td>\n",
       "      <td>178.829</td>\n",
       "      <td>özgür bunlar normalde kamyon daha büyük araçla...</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180.956</td>\n",
       "      <td>184.236</td>\n",
       "      <td>burcu arka tarafı bağlamak kolay olmayacak</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>193.434</td>\n",
       "      <td>196.327</td>\n",
       "      <td>özgür arabaya yarım tur attıracağım</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>197.235</td>\n",
       "      <td>201.338</td>\n",
       "      <td>burcu biraz daha devam et devam et tamam oldu</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>207.806</td>\n",
       "      <td>213.383</td>\n",
       "      <td>burcu şimdilik iki tekere takacağız ama kar ka...</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036932</th>\n",
       "      <td>471.970</td>\n",
       "      <td>472.470</td>\n",
       "      <td>umarız ki bu büyük ve güçlü teknoloji yanlış e...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036933</th>\n",
       "      <td>472.470</td>\n",
       "      <td>482.304</td>\n",
       "      <td>daha faydalı ve özgün kurumlarda herkesin eşit...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036934</th>\n",
       "      <td>482.498</td>\n",
       "      <td>484.178</td>\n",
       "      <td>i zlediğiniz için teşekkürler</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036935</th>\n",
       "      <td>484.178</td>\n",
       "      <td>488.089</td>\n",
       "      <td>yararlandığım kaynakları açıklamada link olara...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036936</th>\n",
       "      <td>488.089</td>\n",
       "      <td>491.770</td>\n",
       "      <td>i leri düzey okuma ve araştırma yapmak isteyen...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3036937 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         start_time  end_time  \\\n",
       "0           171.948   178.829   \n",
       "1           180.956   184.236   \n",
       "2           193.434   196.327   \n",
       "3           197.235   201.338   \n",
       "4           207.806   213.383   \n",
       "...             ...       ...   \n",
       "3036932     471.970   472.470   \n",
       "3036933     472.470   482.304   \n",
       "3036934     482.498   484.178   \n",
       "3036935     484.178   488.089   \n",
       "3036936     488.089   491.770   \n",
       "\n",
       "                                                  sentence     video_id  \n",
       "0        özgür bunlar normalde kamyon daha büyük araçla...  8V9tq1pe8eI  \n",
       "1               burcu arka tarafı bağlamak kolay olmayacak  8V9tq1pe8eI  \n",
       "2                      özgür arabaya yarım tur attıracağım  8V9tq1pe8eI  \n",
       "3            burcu biraz daha devam et devam et tamam oldu  8V9tq1pe8eI  \n",
       "4        burcu şimdilik iki tekere takacağız ama kar ka...  8V9tq1pe8eI  \n",
       "...                                                    ...          ...  \n",
       "3036932  umarız ki bu büyük ve güçlü teknoloji yanlış e...  YFFJ5FyZj4Q  \n",
       "3036933  daha faydalı ve özgün kurumlarda herkesin eşit...  YFFJ5FyZj4Q  \n",
       "3036934                      i zlediğiniz için teşekkürler  YFFJ5FyZj4Q  \n",
       "3036935  yararlandığım kaynakları açıklamada link olara...  YFFJ5FyZj4Q  \n",
       "3036936  i leri düzey okuma ve araştırma yapmak isteyen...  YFFJ5FyZj4Q  \n",
       "\n",
       "[3036937 rows x 4 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_youtube_sentence['start_time'] = df_youtube_sentence['start_time'].apply(lambda x: x.total_seconds()) # convert seconds\n",
    "df_youtube_sentence['end_time'] = df_youtube_sentence['end_time'].apply(lambda x: x.total_seconds())\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>twogram_pair1</th>\n",
       "      <th>twogram_pair2</th>\n",
       "      <th>twogram_pair3</th>\n",
       "      <th>twogram_pair4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bir</td>\n",
       "      <td>bir doktor</td>\n",
       "      <td>bir polis</td>\n",
       "      <td>bir dolar</td>\n",
       "      <td>bir telefon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bu</td>\n",
       "      <td>bu doktor</td>\n",
       "      <td>bu komik</td>\n",
       "      <td>bu telefon</td>\n",
       "      <td>bu bebek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ne</td>\n",
       "      <td>ne şans</td>\n",
       "      <td>mesaj ne</td>\n",
       "      <td>plan ne</td>\n",
       "      <td>ne sürpriz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ve</td>\n",
       "      <td>ve müzik</td>\n",
       "      <td>ve şeker</td>\n",
       "      <td>ve patates</td>\n",
       "      <td>ve çikolata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>için</td>\n",
       "      <td>kontrol için</td>\n",
       "      <td>bebek için</td>\n",
       "      <td>dans için</td>\n",
       "      <td>film için</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mi</td>\n",
       "      <td>komik mi</td>\n",
       "      <td>normal mi</td>\n",
       "      <td>problem mi</td>\n",
       "      <td>profesyonel mi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>o</td>\n",
       "      <td>o ajan</td>\n",
       "      <td>o müzik</td>\n",
       "      <td>o park</td>\n",
       "      <td>o film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ben</td>\n",
       "      <td>ben senatör</td>\n",
       "      <td>ben ajan</td>\n",
       "      <td>ben general</td>\n",
       "      <td>ben profesör</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>de</td>\n",
       "      <td>patates de</td>\n",
       "      <td>şef de</td>\n",
       "      <td>profesör de</td>\n",
       "      <td>prenses de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>çok</td>\n",
       "      <td>çok komik</td>\n",
       "      <td>çok normal</td>\n",
       "      <td>çok pardon</td>\n",
       "      <td>çok süper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ama</td>\n",
       "      <td>ama kaptan</td>\n",
       "      <td>pardon ama</td>\n",
       "      <td>ama patron</td>\n",
       "      <td>ama şef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>var</td>\n",
       "      <td>kontrol var</td>\n",
       "      <td>polis var</td>\n",
       "      <td>bebek var</td>\n",
       "      <td>mesaj var</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>evet</td>\n",
       "      <td>evet kaptan</td>\n",
       "      <td>evet normal</td>\n",
       "      <td>evet patron</td>\n",
       "      <td>evet general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mı</td>\n",
       "      <td>fotoğraf mı</td>\n",
       "      <td>şans mı</td>\n",
       "      <td>bira mı</td>\n",
       "      <td>pizza mı</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>değil</td>\n",
       "      <td>doktor değil</td>\n",
       "      <td>komik değil</td>\n",
       "      <td>dans değil</td>\n",
       "      <td>normal değil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>da</td>\n",
       "      <td>pardon da</td>\n",
       "      <td>avukat da</td>\n",
       "      <td>fotoğraf da</td>\n",
       "      <td>banka da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>şey</td>\n",
       "      <td>komik şey</td>\n",
       "      <td>romantik şey</td>\n",
       "      <td>manyak şey</td>\n",
       "      <td>popüler şey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hayır</td>\n",
       "      <td>hayır kaptan</td>\n",
       "      <td>hayır patron</td>\n",
       "      <td>hayır şef</td>\n",
       "      <td>hayır madam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>daha</td>\n",
       "      <td>daha komik</td>\n",
       "      <td>daha bebek</td>\n",
       "      <td>daha normal</td>\n",
       "      <td>daha romantik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sen</td>\n",
       "      <td>sen doktorsun</td>\n",
       "      <td>sen komiksin</td>\n",
       "      <td>sen süpersin</td>\n",
       "      <td>sen manyaksın</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kadar</td>\n",
       "      <td>okula kadar</td>\n",
       "      <td>filme kadar</td>\n",
       "      <td>parka kadar</td>\n",
       "      <td>trene kadar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bana</td>\n",
       "      <td>telefon bana</td>\n",
       "      <td>bana kahve</td>\n",
       "      <td>bana viski</td>\n",
       "      <td>çikolatalı bana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>yok</td>\n",
       "      <td>polis yok</td>\n",
       "      <td>bebek yok</td>\n",
       "      <td>şans yok</td>\n",
       "      <td>kahve yok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>onu</td>\n",
       "      <td>onu steril</td>\n",
       "      <td>onu kontrol</td>\n",
       "      <td>onu şarj</td>\n",
       "      <td>onu rapor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>seni</td>\n",
       "      <td>seni hipnotize</td>\n",
       "      <td>seni embesil</td>\n",
       "      <td>direkt seni</td>\n",
       "      <td>seni dezenfekte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>beni</td>\n",
       "      <td>beni analiz</td>\n",
       "      <td>beni diskalifiye</td>\n",
       "      <td>beni anons</td>\n",
       "      <td>beni şok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bunu</td>\n",
       "      <td>bunu dizayn</td>\n",
       "      <td>bunu deşifre</td>\n",
       "      <td>bunu izole</td>\n",
       "      <td>bunu arşivle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>gibi</td>\n",
       "      <td>polis gibi</td>\n",
       "      <td>telefon gibi</td>\n",
       "      <td>bebek gibi</td>\n",
       "      <td>dans gibi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word   twogram_pair1     twogram_pair2 twogram_pair3    twogram_pair4\n",
       "0     bir      bir doktor         bir polis     bir dolar      bir telefon\n",
       "1      bu       bu doktor          bu komik    bu telefon         bu bebek\n",
       "2      ne         ne şans          mesaj ne       plan ne       ne sürpriz\n",
       "3      ve        ve müzik          ve şeker    ve patates      ve çikolata\n",
       "4    için    kontrol için        bebek için     dans için        film için\n",
       "5      mi        komik mi         normal mi    problem mi   profesyonel mi\n",
       "6       o          o ajan           o müzik        o park           o film\n",
       "7     ben     ben senatör          ben ajan   ben general     ben profesör\n",
       "8      de      patates de            şef de   profesör de       prenses de\n",
       "9     çok       çok komik        çok normal    çok pardon        çok süper\n",
       "10    ama      ama kaptan        pardon ama    ama patron          ama şef\n",
       "11    var     kontrol var         polis var     bebek var        mesaj var\n",
       "12   evet     evet kaptan       evet normal   evet patron     evet general\n",
       "13     mı     fotoğraf mı           şans mı       bira mı         pizza mı\n",
       "14  değil    doktor değil       komik değil    dans değil     normal değil\n",
       "15     da       pardon da         avukat da   fotoğraf da         banka da\n",
       "16    şey       komik şey      romantik şey    manyak şey      popüler şey\n",
       "17  hayır    hayır kaptan      hayır patron     hayır şef      hayır madam\n",
       "18   daha      daha komik        daha bebek   daha normal    daha romantik\n",
       "19    sen   sen doktorsun      sen komiksin  sen süpersin    sen manyaksın\n",
       "20  kadar     okula kadar       filme kadar   parka kadar      trene kadar\n",
       "21   bana    telefon bana        bana kahve    bana viski  çikolatalı bana\n",
       "22    yok       polis yok         bebek yok      şans yok        kahve yok\n",
       "23    onu      onu steril       onu kontrol      onu şarj        onu rapor\n",
       "24   seni  seni hipnotize      seni embesil   direkt seni  seni dezenfekte\n",
       "25   beni     beni analiz  beni diskalifiye    beni anons         beni şok\n",
       "26   bunu     bunu dizayn      bunu deşifre    bunu izole     bunu arşivle\n",
       "27   gibi      polis gibi      telefon gibi    bebek gibi        dans gibi"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_group_select = pd.read_excel(\"Turkish English Manual Selected 2 Gram Hybrids.xlsx\", sheet_name= f\"{sheets}\")\n",
    "df_word_group_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_list = []\n",
    "for i in range(len(df_word_group_select)):\n",
    "    for j in df_word_group_select.columns[1:]:\n",
    "        string = df_word_group_select.loc[i,j]\n",
    "        search_list.append(string.strip())\n",
    "#search_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_group_youtube(df, search_list, target_column, sample_num): \n",
    "    df_search_result = pd.DataFrame()\n",
    "    for j in search_list:\n",
    "        df_result = df[df[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){j}(?:\\s|$)\", na=True)]\n",
    "        df_result.sort_values(f\"{target_column}\",key=lambda x:x.str.len(), inplace=True)\n",
    "        df_select = df_result.head(sample_num)\n",
    "        df_select.insert(0,\"search_string\",j)\n",
    "        df_search_result = pd.concat([df_search_result,df_select], axis=0)\n",
    "    df_search_result.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return df_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_search_result = pd.DataFrame()\n",
    "#for i in range(len(df_english_select)):\n",
    "#    for j in df_english_select.columns[1:]:\n",
    "#        string = df_english_select.loc[i,j]\n",
    "#        df_result = df_youtube_sent[df_youtube_sent.sentence.str.contains(fr\"(?:\\s|^){string}(?:\\s|$)\", na=True)]\n",
    "#        df_result.sort_values(\"sentence\",key=lambda x:x.str.len(), inplace=True)\n",
    "#        df_select = df_result.head(6)\n",
    "#        df_select.insert(0,\"search_string\",string)\n",
    "#        df_search_result = pd.concat([df_search_result,df_select], axis=0)\n",
    "#df_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kurubal/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_string</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>sentence</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>5313.584</td>\n",
       "      <td>5315.071</td>\n",
       "      <td>i yi bir doktor</td>\n",
       "      <td>BUxpgSVmM2Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>1814.845</td>\n",
       "      <td>1816.165</td>\n",
       "      <td>bir doktor bulduk</td>\n",
       "      <td>HWxPjFUR2Qg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>5310.838</td>\n",
       "      <td>5313.534</td>\n",
       "      <td>bir doktor buldum</td>\n",
       "      <td>BUxpgSVmM2Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>1601.980</td>\n",
       "      <td>1603.200</td>\n",
       "      <td>bir doktor daha he</td>\n",
       "      <td>7VNGhuCG6ps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>8617.840</td>\n",
       "      <td>8618.742</td>\n",
       "      <td>bir doktor baksın ya</td>\n",
       "      <td>WCaHjQI0SKc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>bebek gibi</td>\n",
       "      <td>358.923</td>\n",
       "      <td>360.666</td>\n",
       "      <td>aa ne güzel bebek gibi</td>\n",
       "      <td>KHMNiXYbdaU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>bebek gibi</td>\n",
       "      <td>251.440</td>\n",
       "      <td>252.680</td>\n",
       "      <td>taş bebek gibi olmuşsun</td>\n",
       "      <td>kKVhVDWRrsM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>bebek gibi</td>\n",
       "      <td>305.320</td>\n",
       "      <td>306.440</td>\n",
       "      <td>peluş bebek gibi oluruz</td>\n",
       "      <td>jGMsIgTA_wk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>dans gibi</td>\n",
       "      <td>36.000</td>\n",
       "      <td>43.080</td>\n",
       "      <td>bu nedenle performansım bir dans gibi değil da...</td>\n",
       "      <td>JOGsTT3hDRo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>dans gibi</td>\n",
       "      <td>104.587</td>\n",
       "      <td>112.883</td>\n",
       "      <td>peki sen gerçek anlamda böyle beden eğitimi gi...</td>\n",
       "      <td>4aN0e5dOzh0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>441 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    search_string  start_time  end_time  \\\n",
       "0      bir doktor    5313.584  5315.071   \n",
       "1      bir doktor    1814.845  1816.165   \n",
       "2      bir doktor    5310.838  5313.534   \n",
       "3      bir doktor    1601.980  1603.200   \n",
       "4      bir doktor    8617.840  8618.742   \n",
       "..            ...         ...       ...   \n",
       "436    bebek gibi     358.923   360.666   \n",
       "437    bebek gibi     251.440   252.680   \n",
       "438    bebek gibi     305.320   306.440   \n",
       "439     dans gibi      36.000    43.080   \n",
       "440     dans gibi     104.587   112.883   \n",
       "\n",
       "                                              sentence     video_id  \n",
       "0                                      i yi bir doktor  BUxpgSVmM2Y  \n",
       "1                                    bir doktor bulduk  HWxPjFUR2Qg  \n",
       "2                                    bir doktor buldum  BUxpgSVmM2Y  \n",
       "3                                   bir doktor daha he  7VNGhuCG6ps  \n",
       "4                                 bir doktor baksın ya  WCaHjQI0SKc  \n",
       "..                                                 ...          ...  \n",
       "436                             aa ne güzel bebek gibi  KHMNiXYbdaU  \n",
       "437                            taş bebek gibi olmuşsun  kKVhVDWRrsM  \n",
       "438                            peluş bebek gibi oluruz  jGMsIgTA_wk  \n",
       "439  bu nedenle performansım bir dans gibi değil da...  JOGsTT3hDRo  \n",
       "440  peki sen gerçek anlamda böyle beden eğitimi gi...  4aN0e5dOzh0  \n",
       "\n",
       "[441 rows x 5 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_search_result = word_group_youtube(df_youtube_sentence, search_list, \"sentence\", 6)\n",
    "df_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_time_loc_list = []\n",
    "def word_group_time_loc(df, search, start_sent, end_sent, sent, sent_video_id):\n",
    "    for i in range(len(df)):\n",
    "        word = df.loc[i,f\"{search}\"]\n",
    "        start_time = df.loc[i,f\"{start_sent}\"]\n",
    "        end_time = df.loc[i,f\"{end_sent}\"]\n",
    "        sentence = df.loc[i,f\"{sent}\"]\n",
    "        video_id = df.loc[i,f\"{sent_video_id}\"]\n",
    "        time_length = end_time-start_time\n",
    "        sentence_length = len(sentence)\n",
    "        time_length_ratio = time_length/sentence_length\n",
    "        loc_list = []\n",
    "        for j in re.finditer(fr\"(?:\\s|^){word}(?:\\s|$)\", sentence, re.IGNORECASE|re.UNICODE):\n",
    "            loc_list.append(j)\n",
    "            start = loc_list[0].start()\n",
    "            end = loc_list[0].end()\n",
    "            start_loc = start_time+(start*time_length_ratio)\n",
    "            end_loc = start_time+(end*time_length_ratio)\n",
    "        word_time_loc_list.append([word,start_loc,end_loc,sentence,video_id])\n",
    "    df_word_time_loc = pd.DataFrame(word_time_loc_list, columns=[f\"{search}\",f\"{start_sent}\",f\"{end_sent}\",f\"{sent}\",f\"{sent_video_id}\"])\n",
    "\n",
    "    return df_word_time_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_string</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>sentence</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>5313.980533</td>\n",
       "      <td>5315.071000</td>\n",
       "      <td>i yi bir doktor</td>\n",
       "      <td>BUxpgSVmM2Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>1814.845000</td>\n",
       "      <td>1815.699118</td>\n",
       "      <td>bir doktor bulduk</td>\n",
       "      <td>HWxPjFUR2Qg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>5310.838000</td>\n",
       "      <td>5312.582471</td>\n",
       "      <td>bir doktor buldum</td>\n",
       "      <td>BUxpgSVmM2Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>1601.980000</td>\n",
       "      <td>1602.725556</td>\n",
       "      <td>bir doktor daha he</td>\n",
       "      <td>7VNGhuCG6ps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>8617.840000</td>\n",
       "      <td>8618.336100</td>\n",
       "      <td>bir doktor baksın ya</td>\n",
       "      <td>WCaHjQI0SKc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>bebek gibi</td>\n",
       "      <td>359.794500</td>\n",
       "      <td>360.666000</td>\n",
       "      <td>aa ne güzel bebek gibi</td>\n",
       "      <td>KHMNiXYbdaU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>bebek gibi</td>\n",
       "      <td>251.601739</td>\n",
       "      <td>252.248696</td>\n",
       "      <td>taş bebek gibi olmuşsun</td>\n",
       "      <td>kKVhVDWRrsM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>bebek gibi</td>\n",
       "      <td>305.563478</td>\n",
       "      <td>306.147826</td>\n",
       "      <td>peluş bebek gibi oluruz</td>\n",
       "      <td>jGMsIgTA_wk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>dans gibi</td>\n",
       "      <td>38.331220</td>\n",
       "      <td>39.280976</td>\n",
       "      <td>bu nedenle performansım bir dans gibi değil da...</td>\n",
       "      <td>JOGsTT3hDRo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>dans gibi</td>\n",
       "      <td>108.453097</td>\n",
       "      <td>109.339078</td>\n",
       "      <td>peki sen gerçek anlamda böyle beden eğitimi gi...</td>\n",
       "      <td>4aN0e5dOzh0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>441 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    search_string   start_time     end_time  \\\n",
       "0      bir doktor  5313.980533  5315.071000   \n",
       "1      bir doktor  1814.845000  1815.699118   \n",
       "2      bir doktor  5310.838000  5312.582471   \n",
       "3      bir doktor  1601.980000  1602.725556   \n",
       "4      bir doktor  8617.840000  8618.336100   \n",
       "..            ...          ...          ...   \n",
       "436    bebek gibi   359.794500   360.666000   \n",
       "437    bebek gibi   251.601739   252.248696   \n",
       "438    bebek gibi   305.563478   306.147826   \n",
       "439     dans gibi    38.331220    39.280976   \n",
       "440     dans gibi   108.453097   109.339078   \n",
       "\n",
       "                                              sentence     video_id  \n",
       "0                                      i yi bir doktor  BUxpgSVmM2Y  \n",
       "1                                    bir doktor bulduk  HWxPjFUR2Qg  \n",
       "2                                    bir doktor buldum  BUxpgSVmM2Y  \n",
       "3                                   bir doktor daha he  7VNGhuCG6ps  \n",
       "4                                 bir doktor baksın ya  WCaHjQI0SKc  \n",
       "..                                                 ...          ...  \n",
       "436                             aa ne güzel bebek gibi  KHMNiXYbdaU  \n",
       "437                            taş bebek gibi olmuşsun  kKVhVDWRrsM  \n",
       "438                            peluş bebek gibi oluruz  jGMsIgTA_wk  \n",
       "439  bu nedenle performansım bir dans gibi değil da...  JOGsTT3hDRo  \n",
       "440  peki sen gerçek anlamda böyle beden eğitimi gi...  4aN0e5dOzh0  \n",
       "\n",
       "[441 rows x 5 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_group_time_loc_result = word_group_time_loc(df_search_result, \"search_string\", \"start_time\", \"end_time\", \"sentence\", \"video_id\")\n",
    "df_word_group_time_loc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_string</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>sentence</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>5313.4</td>\n",
       "      <td>5316.6</td>\n",
       "      <td>i yi bir doktor</td>\n",
       "      <td>BUxpgSVmM2Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>1813.4</td>\n",
       "      <td>1816.6</td>\n",
       "      <td>bir doktor bulduk</td>\n",
       "      <td>HWxPjFUR2Qg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>5309.4</td>\n",
       "      <td>5314.6</td>\n",
       "      <td>bir doktor buldum</td>\n",
       "      <td>BUxpgSVmM2Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>1601.4</td>\n",
       "      <td>1604.6</td>\n",
       "      <td>bir doktor daha he</td>\n",
       "      <td>7VNGhuCG6ps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>8617.4</td>\n",
       "      <td>8620.6</td>\n",
       "      <td>bir doktor baksın ya</td>\n",
       "      <td>WCaHjQI0SKc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>bebek gibi</td>\n",
       "      <td>357.4</td>\n",
       "      <td>362.6</td>\n",
       "      <td>aa ne güzel bebek gibi</td>\n",
       "      <td>KHMNiXYbdaU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>bebek gibi</td>\n",
       "      <td>249.4</td>\n",
       "      <td>254.6</td>\n",
       "      <td>taş bebek gibi olmuşsun</td>\n",
       "      <td>kKVhVDWRrsM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>bebek gibi</td>\n",
       "      <td>303.4</td>\n",
       "      <td>306.6</td>\n",
       "      <td>peluş bebek gibi oluruz</td>\n",
       "      <td>jGMsIgTA_wk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>dans gibi</td>\n",
       "      <td>37.4</td>\n",
       "      <td>40.6</td>\n",
       "      <td>bu nedenle performansım bir dans gibi değil da...</td>\n",
       "      <td>JOGsTT3hDRo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>dans gibi</td>\n",
       "      <td>107.4</td>\n",
       "      <td>110.6</td>\n",
       "      <td>peki sen gerçek anlamda böyle beden eğitimi gi...</td>\n",
       "      <td>4aN0e5dOzh0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>441 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    search_string  start_time  end_time  \\\n",
       "0      bir doktor      5313.4    5316.6   \n",
       "1      bir doktor      1813.4    1816.6   \n",
       "2      bir doktor      5309.4    5314.6   \n",
       "3      bir doktor      1601.4    1604.6   \n",
       "4      bir doktor      8617.4    8620.6   \n",
       "..            ...         ...       ...   \n",
       "436    bebek gibi       357.4     362.6   \n",
       "437    bebek gibi       249.4     254.6   \n",
       "438    bebek gibi       303.4     306.6   \n",
       "439     dans gibi        37.4      40.6   \n",
       "440     dans gibi       107.4     110.6   \n",
       "\n",
       "                                              sentence     video_id  \n",
       "0                                      i yi bir doktor  BUxpgSVmM2Y  \n",
       "1                                    bir doktor bulduk  HWxPjFUR2Qg  \n",
       "2                                    bir doktor buldum  BUxpgSVmM2Y  \n",
       "3                                   bir doktor daha he  7VNGhuCG6ps  \n",
       "4                                 bir doktor baksın ya  WCaHjQI0SKc  \n",
       "..                                                 ...          ...  \n",
       "436                             aa ne güzel bebek gibi  KHMNiXYbdaU  \n",
       "437                            taş bebek gibi olmuşsun  kKVhVDWRrsM  \n",
       "438                            peluş bebek gibi oluruz  jGMsIgTA_wk  \n",
       "439  bu nedenle performansım bir dans gibi değil da...  JOGsTT3hDRo  \n",
       "440  peki sen gerçek anlamda böyle beden eğitimi gi...  4aN0e5dOzh0  \n",
       "\n",
       "[441 rows x 5 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#time_shift = 0.3\n",
    "df_word_group_time_loc_result.start_time = df_word_group_time_loc_result.start_time.apply(lambda x: (x-time_shift))\n",
    "df_word_group_time_loc_result.end_time = df_word_group_time_loc_result.end_time.apply(lambda x: (x+time_shift))\n",
    "df_word_group_time_loc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_string</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>sentence</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>5313</td>\n",
       "      <td>5317</td>\n",
       "      <td>i yi bir doktor</td>\n",
       "      <td>BUxpgSVmM2Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>1813</td>\n",
       "      <td>1817</td>\n",
       "      <td>bir doktor bulduk</td>\n",
       "      <td>HWxPjFUR2Qg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>5309</td>\n",
       "      <td>5315</td>\n",
       "      <td>bir doktor buldum</td>\n",
       "      <td>BUxpgSVmM2Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>1601</td>\n",
       "      <td>1605</td>\n",
       "      <td>bir doktor daha he</td>\n",
       "      <td>7VNGhuCG6ps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bir doktor</td>\n",
       "      <td>8617</td>\n",
       "      <td>8621</td>\n",
       "      <td>bir doktor baksın ya</td>\n",
       "      <td>WCaHjQI0SKc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>bebek gibi</td>\n",
       "      <td>357</td>\n",
       "      <td>363</td>\n",
       "      <td>aa ne güzel bebek gibi</td>\n",
       "      <td>KHMNiXYbdaU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>bebek gibi</td>\n",
       "      <td>249</td>\n",
       "      <td>255</td>\n",
       "      <td>taş bebek gibi olmuşsun</td>\n",
       "      <td>kKVhVDWRrsM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>bebek gibi</td>\n",
       "      <td>303</td>\n",
       "      <td>307</td>\n",
       "      <td>peluş bebek gibi oluruz</td>\n",
       "      <td>jGMsIgTA_wk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>dans gibi</td>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "      <td>bu nedenle performansım bir dans gibi değil da...</td>\n",
       "      <td>JOGsTT3hDRo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>dans gibi</td>\n",
       "      <td>107</td>\n",
       "      <td>111</td>\n",
       "      <td>peki sen gerçek anlamda böyle beden eğitimi gi...</td>\n",
       "      <td>4aN0e5dOzh0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>441 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    search_string  start_time  end_time  \\\n",
       "0      bir doktor        5313      5317   \n",
       "1      bir doktor        1813      1817   \n",
       "2      bir doktor        5309      5315   \n",
       "3      bir doktor        1601      1605   \n",
       "4      bir doktor        8617      8621   \n",
       "..            ...         ...       ...   \n",
       "436    bebek gibi         357       363   \n",
       "437    bebek gibi         249       255   \n",
       "438    bebek gibi         303       307   \n",
       "439     dans gibi          37        41   \n",
       "440     dans gibi         107       111   \n",
       "\n",
       "                                              sentence     video_id  \n",
       "0                                      i yi bir doktor  BUxpgSVmM2Y  \n",
       "1                                    bir doktor bulduk  HWxPjFUR2Qg  \n",
       "2                                    bir doktor buldum  BUxpgSVmM2Y  \n",
       "3                                   bir doktor daha he  7VNGhuCG6ps  \n",
       "4                                 bir doktor baksın ya  WCaHjQI0SKc  \n",
       "..                                                 ...          ...  \n",
       "436                             aa ne güzel bebek gibi  KHMNiXYbdaU  \n",
       "437                            taş bebek gibi olmuşsun  kKVhVDWRrsM  \n",
       "438                            peluş bebek gibi oluruz  jGMsIgTA_wk  \n",
       "439  bu nedenle performansım bir dans gibi değil da...  JOGsTT3hDRo  \n",
       "440  peki sen gerçek anlamda böyle beden eğitimi gi...  4aN0e5dOzh0  \n",
       "\n",
       "[441 rows x 5 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_group_time_loc_result.start_time = df_word_group_time_loc_result.start_time.apply(lambda x: round(x))\n",
    "df_word_group_time_loc_result.end_time = df_word_group_time_loc_result.end_time.apply(lambda x: round(x))\n",
    "df_word_group_time_loc_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_result.to_excel(\"Youtube_Time_shift.xlsx\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "651d507d70892fab0fc6529d935cd476f6e2eb1791525b76da6cc8da34bc0503"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
